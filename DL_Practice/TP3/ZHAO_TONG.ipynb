{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small data and deep learning\n",
    "This mini-project proposes to study several techniques for improving challenging context, in which few data and resources are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Assume we are in a context where few \"gold\" labeled data are available for training, say $\\mathcal{X}_{\\text{train}}\\triangleq\\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$, where $N_{\\text{train}}$ is small. A large test set $\\mathcal{X}_{\\text{test}}$ is available. A large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
    "\n",
    "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question:\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
    "|------|------|------|------|\n",
    "|   XXX  | XXX | XXX | XXX |\n",
    "\n",
    "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset)\n",
    "\n",
    "In your final report, please keep the logs of each training procedure you used. We will only run this jupyter if we have some doubts on your implementation. \n",
    "\n",
    "__The total file sizes should not exceed 2MB. Please name your notebook (LASTNAME)\\_(FIRSTNAME).ipynb, zip/tar it with any necessary files required to run your notebook, in a compressed file named (LASTNAME)\\_(FIRSTNAME).X where X is the corresponding extension. Zip/tar files exceeding 2MB will not be considered for grading. Submit the compressed file via the submission link provided on the website of the class.__\n",
    "\n",
    "You can use https://colab.research.google.com/ to run your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library Dependency**: Numpy, PIL, matplotlib, torch, tqdm, torchsummary (for debug), tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import tensorboardX\n",
    "\n",
    "from torchvision.datasets.utils import check_integrity, download_url\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set creation\n",
    "__Question 1:__ Propose a dataloader or modify the file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py in order to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following class can be used for three purposes:\n",
    "> * Get a train dataset with at most n examples (train = 1, max_num = n)\n",
    "> * Get the whole CIFAR10 dataset of size 60000 (train = -1)\n",
    "> * Get the test dataset of size 10000 (train = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10(data.Dataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'batches.meta',\n",
    "        'key': 'label_names',\n",
    "        'md5': '5ff9c542aee3614f3951f8cda6e48888',\n",
    "    }\n",
    "\n",
    "    def __init__(self, root, train=1, max_num = 50000,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if self.train == 1:\n",
    "            downloaded_list = self.train_list\n",
    "        elif self.train == 0:\n",
    "            downloaded_list = self.test_list\n",
    "        elif self.train == -1:\n",
    "            downloaded_list = self.train_list + self.test_list\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        for file_name, checksum in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(f)\n",
    "                else:\n",
    "                    entry = pickle.load(f, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.targets.extend(entry['labels'])\n",
    "                else:\n",
    "                    self.targets.extend(entry['fine_labels'])\n",
    "\n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "        \n",
    "        if len(self.data) > max_num:\n",
    "            self.data = self.data[:max_num]\n",
    "            self.targets = self.targets[:max_num]\n",
    "\n",
    "        self._load_meta()\n",
    "\n",
    "    def _load_meta(self):\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
    "        if not check_integrity(path, self.meta['md5']):\n",
    "            raise RuntimeError('Dataset metadata file not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        with open(path, 'rb') as infile:\n",
    "            if sys.version_info[0] == 2:\n",
    "                data = pickle.load(infile)\n",
    "            else:\n",
    "                data = pickle.load(infile, encoding='latin1')\n",
    "            self.classes = data[self.meta['key']]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        root = self.root\n",
    "        for fentry in (self.train_list + self.test_list):\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "        # extract file\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader Loss:  100\n",
      "Test Loader Loss :  10000\n"
     ]
    }
   ],
   "source": [
    "trainset = CIFAR10(\"../data\", train = 1, max_num = 100)\n",
    "testset = CIFAR10(\"../data\", train = 0)\n",
    "\n",
    "print(\"Train Loader Loss: \", len(trainset))\n",
    "print(\"Test Loader Loss : \", len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. The remaining samples correspond to $\\mathcal{X}$. The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing procedure\n",
    "__Question 2:__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since we have few training examples, it is difficult to seperate a validation dataset to help us evaluate the model. Otherwise the model may overfit easily our training dataset.\n",
    "\n",
    "> (1) One possibility is to download some pictures on the internet which correspond to our classes. This dataset can be used as the validation dataset.\n",
    "\n",
    "> (2) K-folder is not widely used in deep learning because of its computational cost. However it is suitable when we have few examples. We can split the whose set into 5 (10) folders and each time we train the model on 4 (9) shares and evaluate it on 1 share. \n",
    "\n",
    "> (3) Instead of fix K folders, we can choose randomly each time a certain number (3 - 5) of examples as the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw approach: the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performances with reported number from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
    "\n",
    "The key ingredients for training a CNN are the batch size, as well as the learning rate schedule, i.e. how to decrease the learning rate as a function of the number of epochs. A possible schedule is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the laerning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
    "\n",
    "You can get some baselines accuracies in this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. Obviously, it is a different context those researchers had access to GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, net, trainloader, optimizer, criterion):\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    with trange(len(trainloader), file=sys.stderr) as t:\n",
    "        net.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            t.update()\n",
    "\n",
    "        t.set_postfix(train_loss = train_loss, train_acc = correct * 100. / total, epoch = epoch + 1)\n",
    "        t.update()\n",
    "        \n",
    "\n",
    "def val_epoch(net, testloader):\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "        \n",
    "    with trange(len(testloader), file=sys.stderr) as t:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                t.update()\n",
    "\n",
    "        t.set_postfix(test_loss = test_loss / total, test_acc = correct * 100. / total)\n",
    "        t.update(1)\n",
    "            \n",
    "    return correct * 100. / total\n",
    "\n",
    "\n",
    "def update_model(net, best_acc, epoch, name = 'ckpt'):\n",
    "    \n",
    "    tqdm.write(\"Saving...\", file=sys.stderr)\n",
    "    state = {'net': net.state_dict(),\n",
    "             'acc': acc,\n",
    "             'epoch': epoch,\n",
    "            }\n",
    "    \n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + name + '.t7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(\"../data\", train = 1, max_num = 100, transform = transform_train)\n",
    "testset = CIFAR10(\"../data\", train = 0, transform = transform_test)\n",
    "cifarset = CIFAR10(\"../data\", train = -1, max_num = 60000, transform = transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 10, shuffle = True, num_workers = 1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = False, num_workers = 1)\n",
    "cifarloader = torch.utils.data.DataLoader(cifarset, batch_size = 64, shuffle = False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 34.89it/s, epoch=1, train_acc=8, train_loss=24.8]                        \n",
      "158it [00:03, 49.60it/s, test_acc=11.7, test_loss=0.0376]                         \n",
      "Saving...\n",
      "11it [00:00, 24.71it/s, epoch=2, train_acc=28, train_loss=21.2]                        \n",
      "11it [00:00, 36.05it/s, epoch=3, train_acc=39, train_loss=19.5]                        \n",
      "11it [00:00, 37.32it/s, epoch=4, train_acc=40, train_loss=16.2]                        \n",
      "11it [00:00, 37.13it/s, epoch=5, train_acc=50, train_loss=14.4]                        \n",
      "11it [00:00, 37.40it/s, epoch=6, train_acc=60, train_loss=12]                        \n",
      "11it [00:00, 38.92it/s, epoch=7, train_acc=76, train_loss=8.11]                        \n",
      "11it [00:00, 40.00it/s, epoch=8, train_acc=78, train_loss=6.15]                        \n",
      "11it [00:00, 37.04it/s, epoch=9, train_acc=86, train_loss=4.32]                        \n",
      "11it [00:00, 27.36it/s, epoch=10, train_acc=86, train_loss=3.69]                        \n",
      "11it [00:00, 36.80it/s, epoch=11, train_acc=90, train_loss=2.66]                        \n",
      "158it [00:03, 48.82it/s, test_acc=20.8, test_loss=0.0579]                         \n",
      "Saving...\n",
      "11it [00:00, 37.28it/s, epoch=12, train_acc=94, train_loss=1.96]                        \n",
      "11it [00:00, 39.98it/s, epoch=13, train_acc=93, train_loss=2.3]                        \n",
      "11it [00:00, 40.45it/s, epoch=14, train_acc=95, train_loss=2.25]                        \n",
      "11it [00:00, 38.80it/s, epoch=15, train_acc=99, train_loss=0.992]                        \n",
      "11it [00:00, 39.77it/s, epoch=16, train_acc=98, train_loss=0.772]                        \n",
      "11it [00:00, 40.02it/s, epoch=17, train_acc=100, train_loss=0.331]                        \n",
      "11it [00:00, 38.50it/s, epoch=18, train_acc=99, train_loss=0.718]                        \n",
      "11it [00:00, 39.76it/s, epoch=19, train_acc=99, train_loss=0.521]                        \n",
      "11it [00:00, 39.48it/s, epoch=20, train_acc=99, train_loss=0.417]                        \n",
      "11it [00:00, 39.99it/s, epoch=21, train_acc=100, train_loss=0.194]                        \n",
      "158it [00:03, 52.42it/s, test_acc=21.7, test_loss=0.0685]                         \n",
      "Saving...\n",
      "11it [00:00, 37.13it/s, epoch=22, train_acc=100, train_loss=0.286]                        \n",
      "11it [00:00, 39.28it/s, epoch=23, train_acc=100, train_loss=0.0762]                        \n",
      "11it [00:00, 38.92it/s, epoch=24, train_acc=100, train_loss=0.0994]                        \n",
      "11it [00:00, 39.63it/s, epoch=25, train_acc=100, train_loss=0.122]                        \n",
      "11it [00:00, 38.91it/s, epoch=26, train_acc=100, train_loss=0.0629]                        \n",
      "11it [00:00, 38.51it/s, epoch=27, train_acc=100, train_loss=0.0364]                        \n",
      "11it [00:00, 39.09it/s, epoch=28, train_acc=100, train_loss=0.0802]                        \n",
      "11it [00:00, 40.11it/s, epoch=29, train_acc=100, train_loss=0.0514]                        \n",
      "11it [00:00, 39.67it/s, epoch=30, train_acc=100, train_loss=0.0333]                        \n",
      "11it [00:00, 36.32it/s, epoch=31, train_acc=100, train_loss=0.0265]                        \n",
      "158it [00:03, 52.24it/s, test_acc=23.2, test_loss=0.0687]                         \n",
      "Saving...\n",
      "11it [00:00, 36.98it/s, epoch=32, train_acc=100, train_loss=0.0341]                        \n",
      "11it [00:00, 40.08it/s, epoch=33, train_acc=100, train_loss=0.0253]                        \n",
      "11it [00:00, 40.26it/s, epoch=34, train_acc=100, train_loss=0.0273]                        \n",
      "11it [00:00, 39.87it/s, epoch=35, train_acc=100, train_loss=0.0265]                        \n",
      "11it [00:00, 39.56it/s, epoch=36, train_acc=100, train_loss=0.0422]                        \n",
      "11it [00:00, 39.85it/s, epoch=37, train_acc=100, train_loss=0.0271]                        \n",
      "11it [00:00, 39.44it/s, epoch=38, train_acc=100, train_loss=0.0152]                        \n",
      "11it [00:00, 39.82it/s, epoch=39, train_acc=100, train_loss=0.0183]                        \n",
      "11it [00:00, 39.98it/s, epoch=40, train_acc=100, train_loss=0.0117]                        \n",
      "11it [00:00, 39.80it/s, epoch=41, train_acc=100, train_loss=0.0689]                        \n",
      "158it [00:03, 51.75it/s, test_acc=23.3, test_loss=0.0687]                         \n",
      "Saving...\n",
      "11it [00:00, 25.60it/s, epoch=42, train_acc=100, train_loss=0.0228]                        \n",
      "11it [00:00, 39.30it/s, epoch=43, train_acc=100, train_loss=0.0309]                        \n",
      "11it [00:00, 38.53it/s, epoch=44, train_acc=100, train_loss=0.0301]                        \n",
      "11it [00:00, 35.72it/s, epoch=45, train_acc=100, train_loss=0.0267]                        \n",
      "11it [00:00, 36.17it/s, epoch=46, train_acc=100, train_loss=0.0209]                        \n",
      "11it [00:00, 38.79it/s, epoch=47, train_acc=100, train_loss=0.0122]                        \n",
      "11it [00:00, 36.57it/s, epoch=48, train_acc=100, train_loss=0.0136]                        \n",
      "11it [00:00, 40.01it/s, epoch=49, train_acc=100, train_loss=0.0135]                        \n",
      "11it [00:00, 36.99it/s, epoch=50, train_acc=100, train_loss=0.0143]                        \n",
      "11it [00:00, 36.84it/s, epoch=51, train_acc=100, train_loss=0.0118]                        \n",
      "158it [00:03, 49.95it/s, test_acc=23.8, test_loss=0.0687]                         \n",
      "Saving...\n",
      "11it [00:00, 36.39it/s, epoch=52, train_acc=100, train_loss=0.0114]                        \n",
      "11it [00:00, 38.45it/s, epoch=53, train_acc=100, train_loss=0.0137]                        \n",
      "11it [00:00, 36.72it/s, epoch=54, train_acc=100, train_loss=0.00916]                        \n",
      "11it [00:00, 36.16it/s, epoch=55, train_acc=100, train_loss=0.00972]                        \n",
      "11it [00:00, 37.02it/s, epoch=56, train_acc=100, train_loss=0.0207]                        \n",
      "11it [00:00, 40.00it/s, epoch=57, train_acc=100, train_loss=0.0237]                        \n",
      "11it [00:00, 37.76it/s, epoch=58, train_acc=100, train_loss=0.0407]                        \n",
      "11it [00:00, 36.84it/s, epoch=59, train_acc=100, train_loss=0.00985]                        \n",
      "11it [00:00, 36.62it/s, epoch=60, train_acc=100, train_loss=0.015]                        \n",
      "11it [00:00, 35.55it/s, epoch=61, train_acc=100, train_loss=0.00893]                        \n",
      "158it [00:03, 51.69it/s, test_acc=23.9, test_loss=0.069]                         \n",
      "Saving...\n",
      "11it [00:00, 35.67it/s, epoch=62, train_acc=100, train_loss=0.0126]                        \n",
      "11it [00:00, 39.68it/s, epoch=63, train_acc=100, train_loss=0.0232]                        \n",
      "11it [00:00, 38.67it/s, epoch=64, train_acc=100, train_loss=0.0102]                        \n",
      "11it [00:00, 36.59it/s, epoch=65, train_acc=100, train_loss=0.0125]                        \n",
      "11it [00:00, 39.43it/s, epoch=66, train_acc=100, train_loss=0.0145]                        \n",
      "11it [00:00, 40.02it/s, epoch=67, train_acc=100, train_loss=0.01]                        \n",
      "11it [00:00, 35.55it/s, epoch=68, train_acc=100, train_loss=0.0127]                        \n",
      "11it [00:00, 38.61it/s, epoch=69, train_acc=100, train_loss=0.0066]                        \n",
      "11it [00:00, 35.55it/s, epoch=70, train_acc=100, train_loss=0.0137]                        \n",
      "11it [00:00, 37.03it/s, epoch=71, train_acc=100, train_loss=0.0132]                        \n",
      "158it [00:03, 48.38it/s, test_acc=23.9, test_loss=0.0692]                         \n",
      "Saving...\n",
      "11it [00:00, 36.01it/s, epoch=72, train_acc=100, train_loss=0.0104]                        \n",
      "11it [00:00, 39.60it/s, epoch=73, train_acc=100, train_loss=0.0102]                        \n",
      "11it [00:00, 39.59it/s, epoch=74, train_acc=100, train_loss=0.0107]                        \n",
      "11it [00:00, 40.16it/s, epoch=75, train_acc=100, train_loss=0.0164]                        \n",
      "11it [00:00, 40.19it/s, epoch=76, train_acc=100, train_loss=0.0262]                        \n",
      "11it [00:00, 39.67it/s, epoch=77, train_acc=100, train_loss=0.028]                        \n",
      "11it [00:00, 38.90it/s, epoch=78, train_acc=100, train_loss=0.00927]                        \n",
      "11it [00:00, 40.07it/s, epoch=79, train_acc=100, train_loss=0.00689]                        \n",
      "11it [00:00, 40.06it/s, epoch=80, train_acc=100, train_loss=0.0105]                        \n",
      "11it [00:00, 39.98it/s, epoch=81, train_acc=100, train_loss=0.00798]                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158it [00:03, 52.06it/s, test_acc=24, test_loss=0.0695]                         \n",
      "Saving...\n",
      "11it [00:00, 36.60it/s, epoch=82, train_acc=100, train_loss=0.0121]                        \n",
      "11it [00:00, 40.19it/s, epoch=83, train_acc=100, train_loss=0.0248]                        \n",
      "11it [00:00, 40.14it/s, epoch=84, train_acc=100, train_loss=0.00552]                        \n",
      "11it [00:00, 39.88it/s, epoch=85, train_acc=100, train_loss=0.00917]                        \n",
      "11it [00:00, 39.85it/s, epoch=86, train_acc=100, train_loss=0.0203]                        \n",
      "11it [00:00, 38.00it/s, epoch=87, train_acc=100, train_loss=0.0108]                        \n",
      "11it [00:00, 40.12it/s, epoch=88, train_acc=100, train_loss=0.0347]                        \n",
      "11it [00:00, 39.72it/s, epoch=89, train_acc=100, train_loss=0.0117]                        \n",
      "11it [00:00, 36.03it/s, epoch=90, train_acc=100, train_loss=0.00831]                        \n",
      "11it [00:00, 37.29it/s, epoch=91, train_acc=100, train_loss=0.031]                        \n",
      "158it [00:03, 50.40it/s, test_acc=24, test_loss=0.0698]                         \n",
      "11it [00:00, 36.74it/s, epoch=92, train_acc=100, train_loss=0.00428]                        \n",
      "11it [00:00, 39.45it/s, epoch=93, train_acc=100, train_loss=0.0138]                        \n",
      "11it [00:00, 39.70it/s, epoch=94, train_acc=100, train_loss=0.00916]                        \n",
      "11it [00:00, 37.27it/s, epoch=95, train_acc=100, train_loss=0.00919]                        \n",
      "11it [00:00, 37.31it/s, epoch=96, train_acc=100, train_loss=0.00578]                        \n",
      "11it [00:00, 38.23it/s, epoch=97, train_acc=100, train_loss=0.00627]                        \n",
      "11it [00:00, 39.62it/s, epoch=98, train_acc=100, train_loss=0.00701]                        \n",
      "11it [00:00, 39.05it/s, epoch=99, train_acc=100, train_loss=0.0174]                        \n",
      "11it [00:00, 37.15it/s, epoch=100, train_acc=100, train_loss=0.0244]                        \n"
     ]
    }
   ],
   "source": [
    "# Define net\n",
    "net = ResNet18()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 5e-4)\n",
    "best_acc = 0\n",
    "\n",
    "# Train\n",
    "for epoch in range(100):\n",
    "    \n",
    "    train_epoch(epoch, net, trainloader, optimizer, criterion)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        acc = val_epoch(net, testloader)\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            update_model(net, best_acc, epoch, \"resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 72.23it/s, test_acc=100, test_loss=5.23e-5]                        \n",
      "158it [00:03, 51.93it/s, test_acc=24, test_loss=0.0695]                         \n",
      "939it [00:17, 53.21it/s, test_acc=24.3, test_loss=0.0687]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:  100.0\n",
      "Val   Acc:  23.99\n",
      "Cifar Acc:  24.346666666666668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test model on three datasets\n",
    "net.load_state_dict(torch.load(\"checkpoint/resnet18.t7\")['net'])\n",
    "\n",
    "train_acc = val_epoch(net, trainloader)\n",
    "val_acc = val_epoch(net, testloader)\n",
    "cifar_acc = val_epoch(net, cifarloader)\n",
    "\n",
    "print(\"Train Acc: \", train_acc)\n",
    "print(\"Val   Acc: \", val_acc)\n",
    "print(\"Cifar Acc: \", cifar_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3:__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1409.1556 ). Please report the accuracy obtained on the whole dataset as well as the reference paper/GitHub link.\n",
    "\n",
    "*Hint:* You can re-use the following code: https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (~5 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We test the model on 3 datasets: the train dataset, the test dataset, and the results on shown as below:\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on Full Data | Reference github|\n",
    "|------|------|------|------|------|------|\n",
    "|   ResNet18  | 100 | 100.0 | 24 | 24.35 | https://github.com/kuangliu/pytorch-cifar |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-like architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 45.77it/s, epoch=1, train_acc=9, train_loss=24.3]                        \n",
      "158it [00:01, 85.67it/s, test_acc=16.6, test_loss=0.0381]                         \n",
      "Saving...\n",
      "11it [00:00, 44.27it/s, epoch=2, train_acc=23, train_loss=21.2]                        \n",
      "11it [00:00, 58.60it/s, epoch=3, train_acc=42, train_loss=17.7]                        \n",
      "11it [00:00, 58.33it/s, epoch=4, train_acc=48, train_loss=14.4]                        \n",
      "11it [00:00, 57.83it/s, epoch=5, train_acc=68, train_loss=10.4]                        \n",
      "11it [00:00, 51.96it/s, epoch=6, train_acc=80, train_loss=6.6]                        \n",
      "11it [00:00, 54.03it/s, epoch=7, train_acc=82, train_loss=5.88]                        \n",
      "11it [00:00, 55.73it/s, epoch=8, train_acc=90, train_loss=4.03]                        \n",
      "11it [00:00, 58.84it/s, epoch=9, train_acc=91, train_loss=3.18]                        \n",
      "11it [00:00, 55.24it/s, epoch=10, train_acc=96, train_loss=2.09]                        \n",
      "11it [00:00, 58.70it/s, epoch=11, train_acc=98, train_loss=1.29]                        \n",
      "158it [00:01, 83.39it/s, test_acc=25.7, test_loss=0.0525]                         \n",
      "Saving...\n",
      "11it [00:00, 43.63it/s, epoch=12, train_acc=93, train_loss=1.5]                        \n",
      "11it [00:00, 55.40it/s, epoch=13, train_acc=94, train_loss=1.86]                        \n",
      "11it [00:00, 56.36it/s, epoch=14, train_acc=95, train_loss=1.41]                        \n",
      "11it [00:00, 57.98it/s, epoch=15, train_acc=91, train_loss=2.45]                        \n",
      "11it [00:00, 58.15it/s, epoch=16, train_acc=95, train_loss=1.69]                        \n",
      "11it [00:00, 57.92it/s, epoch=17, train_acc=96, train_loss=1.28]                        \n",
      "11it [00:00, 57.88it/s, epoch=18, train_acc=97, train_loss=1.62]                        \n",
      "11it [00:00, 55.11it/s, epoch=19, train_acc=99, train_loss=0.536]                        \n",
      "11it [00:00, 54.24it/s, epoch=20, train_acc=99, train_loss=0.35]                        \n",
      "11it [00:00, 57.23it/s, epoch=21, train_acc=100, train_loss=0.293]                        \n",
      "158it [00:01, 80.60it/s, test_acc=27.3, test_loss=0.0632]                         \n",
      "Saving...\n",
      "11it [00:00, 45.13it/s, epoch=22, train_acc=100, train_loss=0.138]                        \n",
      "11it [00:00, 58.49it/s, epoch=23, train_acc=100, train_loss=0.0489]                        \n",
      "11it [00:00, 59.11it/s, epoch=24, train_acc=100, train_loss=0.0849]                        \n",
      "11it [00:00, 54.59it/s, epoch=25, train_acc=100, train_loss=0.0231]                        \n",
      "11it [00:00, 57.57it/s, epoch=26, train_acc=100, train_loss=0.07]                        \n",
      "11it [00:00, 57.95it/s, epoch=27, train_acc=100, train_loss=0.0818]                        \n",
      "11it [00:00, 58.91it/s, epoch=28, train_acc=100, train_loss=0.0358]                        \n",
      "11it [00:00, 59.19it/s, epoch=29, train_acc=100, train_loss=0.0785]                        \n",
      "11it [00:00, 57.57it/s, epoch=30, train_acc=100, train_loss=0.0174]                        \n",
      "11it [00:00, 50.89it/s, epoch=31, train_acc=100, train_loss=0.00952]                        \n",
      "158it [00:01, 78.37it/s, test_acc=27.2, test_loss=0.0701]                         \n",
      "11it [00:00, 44.54it/s, epoch=32, train_acc=100, train_loss=0.0144]                        \n",
      "11it [00:00, 58.33it/s, epoch=33, train_acc=100, train_loss=0.027]                        \n",
      "11it [00:00, 56.37it/s, epoch=34, train_acc=100, train_loss=0.0177]                        \n",
      "11it [00:00, 59.04it/s, epoch=35, train_acc=100, train_loss=0.0149]                        \n",
      "11it [00:00, 58.71it/s, epoch=36, train_acc=100, train_loss=0.0115]                        \n",
      "11it [00:00, 58.48it/s, epoch=37, train_acc=100, train_loss=0.0182]                        \n",
      "11it [00:00, 58.88it/s, epoch=38, train_acc=100, train_loss=0.0156]                        \n",
      "11it [00:00, 57.22it/s, epoch=39, train_acc=100, train_loss=0.0142]                        \n",
      "11it [00:00, 58.94it/s, epoch=40, train_acc=100, train_loss=0.0076]                        \n",
      "11it [00:00, 57.78it/s, epoch=41, train_acc=100, train_loss=0.0245]                        \n",
      "158it [00:01, 82.91it/s, test_acc=27.6, test_loss=0.0708]                         \n",
      "Saving...\n",
      "11it [00:00, 45.69it/s, epoch=42, train_acc=100, train_loss=0.00461]                        \n",
      "11it [00:00, 58.60it/s, epoch=43, train_acc=100, train_loss=0.0124]                        \n",
      "11it [00:00, 58.27it/s, epoch=44, train_acc=100, train_loss=0.00732]                        \n",
      "11it [00:00, 57.82it/s, epoch=45, train_acc=100, train_loss=0.00746]                        \n",
      "11it [00:00, 58.73it/s, epoch=46, train_acc=100, train_loss=0.0198]                        \n",
      "11it [00:00, 54.32it/s, epoch=47, train_acc=100, train_loss=0.00863]                        \n",
      "11it [00:00, 55.36it/s, epoch=48, train_acc=100, train_loss=0.00577]                        \n",
      "11it [00:00, 58.27it/s, epoch=49, train_acc=100, train_loss=0.00651]                        \n",
      "11it [00:00, 58.38it/s, epoch=50, train_acc=100, train_loss=0.00805]                        \n",
      "11it [00:00, 57.28it/s, epoch=51, train_acc=100, train_loss=0.00551]                        \n",
      "158it [00:01, 86.40it/s, test_acc=27.3, test_loss=0.0719]                         \n",
      "11it [00:00, 41.97it/s, epoch=52, train_acc=100, train_loss=0.00692]                        \n",
      "11it [00:00, 54.69it/s, epoch=53, train_acc=100, train_loss=0.00563]                        \n",
      "11it [00:00, 58.67it/s, epoch=54, train_acc=100, train_loss=0.0207]                        \n",
      "11it [00:00, 59.13it/s, epoch=55, train_acc=100, train_loss=0.0077]                        \n",
      "11it [00:00, 57.97it/s, epoch=56, train_acc=100, train_loss=0.00715]                        \n",
      "11it [00:00, 58.75it/s, epoch=57, train_acc=100, train_loss=0.00731]                        \n",
      "11it [00:00, 59.47it/s, epoch=58, train_acc=100, train_loss=0.012]                        \n",
      "11it [00:00, 57.11it/s, epoch=59, train_acc=100, train_loss=0.0101]                        \n",
      "11it [00:00, 54.44it/s, epoch=60, train_acc=100, train_loss=0.00582]                        \n",
      "11it [00:00, 53.34it/s, epoch=61, train_acc=100, train_loss=0.0056]                        \n",
      "158it [00:01, 86.09it/s, test_acc=27.7, test_loss=0.0719]                         \n",
      "Saving...\n",
      "11it [00:00, 43.77it/s, epoch=62, train_acc=100, train_loss=0.00855]                        \n",
      "11it [00:00, 58.58it/s, epoch=63, train_acc=100, train_loss=0.00858]                        \n",
      "11it [00:00, 59.58it/s, epoch=64, train_acc=100, train_loss=0.00869]                        \n",
      "11it [00:00, 56.55it/s, epoch=65, train_acc=100, train_loss=0.0496]                        \n",
      "11it [00:00, 58.66it/s, epoch=66, train_acc=100, train_loss=0.00692]                        \n",
      "11it [00:00, 58.32it/s, epoch=67, train_acc=100, train_loss=0.0122]                        \n",
      "11it [00:00, 55.29it/s, epoch=68, train_acc=100, train_loss=0.00978]                        \n",
      "11it [00:00, 58.06it/s, epoch=69, train_acc=100, train_loss=0.0139]                        \n",
      "11it [00:00, 57.89it/s, epoch=70, train_acc=100, train_loss=0.00647]                        \n",
      "11it [00:00, 59.20it/s, epoch=71, train_acc=100, train_loss=0.00391]                        \n",
      "158it [00:01, 84.67it/s, test_acc=27.6, test_loss=0.0731]                         \n",
      "11it [00:00, 43.58it/s, epoch=72, train_acc=100, train_loss=0.00509]                        \n",
      "11it [00:00, 57.54it/s, epoch=73, train_acc=100, train_loss=0.00647]                        \n",
      "11it [00:00, 57.64it/s, epoch=74, train_acc=100, train_loss=0.0153]                        \n",
      "11it [00:00, 58.68it/s, epoch=75, train_acc=100, train_loss=0.0053]                        \n",
      "11it [00:00, 59.28it/s, epoch=76, train_acc=100, train_loss=0.00605]                        \n",
      "11it [00:00, 58.63it/s, epoch=77, train_acc=100, train_loss=0.0063]                        \n",
      "11it [00:00, 58.56it/s, epoch=78, train_acc=100, train_loss=0.00452]                        \n",
      "11it [00:00, 58.90it/s, epoch=79, train_acc=100, train_loss=0.00553]                        \n",
      "11it [00:00, 55.45it/s, epoch=80, train_acc=100, train_loss=0.00594]                        \n",
      "11it [00:00, 56.88it/s, epoch=81, train_acc=100, train_loss=0.00292]                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158it [00:01, 85.49it/s, test_acc=27.5, test_loss=0.0734]                         \n",
      "11it [00:00, 42.66it/s, epoch=82, train_acc=100, train_loss=0.00266]                        \n",
      "11it [00:00, 58.05it/s, epoch=83, train_acc=100, train_loss=0.00489]                        \n",
      "11it [00:00, 56.36it/s, epoch=84, train_acc=100, train_loss=0.00341]                        \n",
      "11it [00:00, 59.51it/s, epoch=85, train_acc=100, train_loss=0.0068]                        \n",
      "11it [00:00, 59.17it/s, epoch=86, train_acc=100, train_loss=0.00348]                        \n",
      "11it [00:00, 60.00it/s, epoch=87, train_acc=100, train_loss=0.00566]                        \n",
      "11it [00:00, 58.62it/s, epoch=88, train_acc=100, train_loss=0.00472]                        \n",
      "11it [00:00, 51.72it/s, epoch=89, train_acc=100, train_loss=0.00371]                        \n",
      "11it [00:00, 53.55it/s, epoch=90, train_acc=100, train_loss=0.00889]                        \n",
      "11it [00:00, 58.07it/s, epoch=91, train_acc=100, train_loss=0.00546]                        \n",
      "158it [00:01, 80.57it/s, test_acc=27.6, test_loss=0.0734]                         \n",
      "11it [00:00, 39.76it/s, epoch=92, train_acc=100, train_loss=0.00429]                        \n",
      "11it [00:00, 59.45it/s, epoch=93, train_acc=100, train_loss=0.00437]                        \n",
      "11it [00:00, 58.57it/s, epoch=94, train_acc=100, train_loss=0.00374]                        \n",
      "11it [00:00, 59.25it/s, epoch=95, train_acc=100, train_loss=0.00318]                        \n",
      "11it [00:00, 57.67it/s, epoch=96, train_acc=100, train_loss=0.00368]                        \n",
      "11it [00:00, 50.64it/s, epoch=97, train_acc=100, train_loss=0.0037]                        \n",
      "11it [00:00, 49.56it/s, epoch=98, train_acc=100, train_loss=0.00456]                        \n",
      "11it [00:00, 54.66it/s, epoch=99, train_acc=100, train_loss=0.0069]                        \n",
      "11it [00:00, 59.65it/s, epoch=100, train_acc=100, train_loss=0.00355]                        \n"
     ]
    }
   ],
   "source": [
    "net = VGG(\"VGG11\")\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.005, momentum = 0.9, weight_decay = 5e-4)\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    train_epoch(epoch, net, trainloader, optimizer, criterion)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        acc = val_epoch(net, testloader)\n",
    "        \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        update_model(net, best_acc, epoch, \"vgg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 107.33it/s, test_acc=100, test_loss=0.000121]                       \n",
      "158it [00:01, 85.32it/s, test_acc=27.7, test_loss=0.0719]                         \n",
      "939it [00:10, 87.91it/s, test_acc=27.6, test_loss=0.0715]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:  100.0\n",
      "Val   Acc:  27.67\n",
      "Cifar Acc:  27.628333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"checkpoint/vgg.t7\")['net'])\n",
    "\n",
    "train_acc = val_epoch(net, trainloader)\n",
    "val_acc = val_epoch(net, testloader)\n",
    "cifar_acc = val_epoch(net, cifarloader)\n",
    "\n",
    "print(\"Train Acc: \", train_acc)\n",
    "print(\"Val   Acc: \", val_acc)\n",
    "print(\"Cifar Acc: \", cifar_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4:__ Same question as before, but with a *VGG*. Which model do you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We test the model on 3 datasets: the train dataset, the test dataset, and the results on shown as below:\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on Full Data | Reference github|\n",
    "|------|------|------|------|------|------|\n",
    "|   VGG11  | 100 | 100.0 | 27.67 | 27.63 | https://github.com/kuangliu/pytorch-cifar |\n",
    "\n",
    "> Compared to ResNet18, VGG11 achieved a slighly better accuracy on both test dataset and full dataset. While ResNet18 is faster than VGG11. Thus if sufficient computational power is available, I recommand VGG, otherwise ResNet is a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "model = models.resnet18(pretrained = True)\n",
    "set_parameter_requires_grad(model, True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(input_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.CenterCrop(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(\"../data\", train = 1, max_num = 100, transform = transform_train)\n",
    "testset = CIFAR10(\"../data\", train = 0, transform = transform_test)\n",
    "cifarset = CIFAR10(\"../data\", train = -1, max_num = 60000, transform = transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True, num_workers = 1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 128, shuffle = False, num_workers = 1)\n",
    "cifarloader = torch.utils.data.DataLoader(cifarset, batch_size = 128, shuffle = False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 15.81it/s, epoch=1, train_acc=15, train_loss=9.37]                       \n",
      "80it [00:15,  5.03it/s, test_acc=11.5, test_loss=0.0195]                        \n",
      "Saving...\n",
      "5it [00:00, 16.47it/s, epoch=2, train_acc=13, train_loss=9.03]                       \n",
      "5it [00:00, 17.72it/s, epoch=3, train_acc=15, train_loss=9.72]                       \n",
      "5it [00:00, 17.90it/s, epoch=4, train_acc=21, train_loss=8.55]                       \n",
      "5it [00:00, 17.92it/s, epoch=5, train_acc=30, train_loss=8.59]                       \n",
      "5it [00:00, 17.96it/s, epoch=6, train_acc=28, train_loss=8.13]                       \n",
      "5it [00:00, 17.90it/s, epoch=7, train_acc=30, train_loss=7.65]                       \n",
      "5it [00:00, 17.69it/s, epoch=8, train_acc=30, train_loss=7.88]                       \n",
      "5it [00:00, 17.79it/s, epoch=9, train_acc=31, train_loss=7.68]                       \n",
      "5it [00:00, 17.73it/s, epoch=10, train_acc=33, train_loss=7.55]                       \n",
      "5it [00:00, 17.56it/s, epoch=11, train_acc=39, train_loss=7.11]                       \n",
      "80it [00:15,  5.12it/s, test_acc=27.8, test_loss=0.0164]                        \n",
      "Saving...\n",
      "5it [00:00, 16.69it/s, epoch=12, train_acc=41, train_loss=7.75]                       \n",
      "5it [00:00, 17.82it/s, epoch=13, train_acc=41, train_loss=7.29]                       \n",
      "5it [00:00, 17.50it/s, epoch=14, train_acc=50, train_loss=6.73]                       \n",
      "5it [00:00, 18.02it/s, epoch=15, train_acc=43, train_loss=6.5]                       \n",
      "5it [00:00, 18.07it/s, epoch=16, train_acc=39, train_loss=7.47]                       \n",
      "5it [00:00, 17.96it/s, epoch=17, train_acc=42, train_loss=7.2]                       \n",
      "5it [00:00, 17.41it/s, epoch=18, train_acc=40, train_loss=6.72]                       \n",
      "5it [00:00, 17.86it/s, epoch=19, train_acc=48, train_loss=6.42]                       \n",
      "5it [00:00, 17.68it/s, epoch=20, train_acc=51, train_loss=6.01]                       \n",
      "5it [00:00, 17.02it/s, epoch=21, train_acc=50, train_loss=5.99]                       \n",
      "80it [00:15,  5.15it/s, test_acc=36.7, test_loss=0.0146]                        \n",
      "Saving...\n",
      "5it [00:00, 16.98it/s, epoch=22, train_acc=50, train_loss=6.73]                       \n",
      "5it [00:00, 17.64it/s, epoch=23, train_acc=43, train_loss=6.89]                       \n",
      "5it [00:00, 17.82it/s, epoch=24, train_acc=49, train_loss=6.52]                       \n",
      "5it [00:00, 17.97it/s, epoch=25, train_acc=49, train_loss=6.18]                       \n",
      "5it [00:00, 18.06it/s, epoch=26, train_acc=55, train_loss=6.23]                       \n",
      "5it [00:00, 17.76it/s, epoch=27, train_acc=47, train_loss=6.04]                       \n",
      "5it [00:00, 17.89it/s, epoch=28, train_acc=56, train_loss=6.23]                       \n",
      "5it [00:00, 17.91it/s, epoch=29, train_acc=65, train_loss=5.58]                       \n",
      "5it [00:00, 18.01it/s, epoch=30, train_acc=53, train_loss=6.5]                       \n",
      "5it [00:00, 17.94it/s, epoch=31, train_acc=58, train_loss=5.96]                       \n",
      "80it [00:15,  5.06it/s, test_acc=43.8, test_loss=0.0134]                        \n",
      "Saving...\n",
      "5it [00:00, 17.00it/s, epoch=32, train_acc=58, train_loss=5.97]                       \n",
      "5it [00:00, 17.88it/s, epoch=33, train_acc=55, train_loss=5.69]                       \n",
      "5it [00:00, 17.87it/s, epoch=34, train_acc=57, train_loss=5.13]                       \n",
      "5it [00:00, 17.50it/s, epoch=35, train_acc=56, train_loss=5.23]                       \n",
      "5it [00:00, 17.13it/s, epoch=36, train_acc=62, train_loss=5.29]                       \n",
      "5it [00:00, 17.61it/s, epoch=37, train_acc=60, train_loss=6.09]                       \n",
      "5it [00:00, 17.97it/s, epoch=38, train_acc=57, train_loss=5.41]                       \n",
      "5it [00:00, 17.74it/s, epoch=39, train_acc=70, train_loss=5.34]                       \n",
      "5it [00:00, 17.46it/s, epoch=40, train_acc=73, train_loss=4.88]                       \n",
      "5it [00:00, 17.77it/s, epoch=41, train_acc=64, train_loss=5.51]                       \n",
      "80it [00:15,  5.11it/s, test_acc=46.8, test_loss=0.0125]                        \n",
      "Saving...\n",
      "5it [00:00, 15.95it/s, epoch=42, train_acc=62, train_loss=5.16]                       \n",
      "5it [00:00, 17.62it/s, epoch=43, train_acc=66, train_loss=5.57]                       \n",
      "5it [00:00, 17.16it/s, epoch=44, train_acc=70, train_loss=5.2]                       \n",
      "5it [00:00, 17.95it/s, epoch=45, train_acc=64, train_loss=5.46]                       \n",
      "5it [00:00, 17.60it/s, epoch=46, train_acc=64, train_loss=4.76]                       \n",
      "5it [00:00, 17.56it/s, epoch=47, train_acc=67, train_loss=5.46]                       \n",
      "5it [00:00, 17.42it/s, epoch=48, train_acc=70, train_loss=4.82]                       \n",
      "5it [00:00, 17.53it/s, epoch=49, train_acc=63, train_loss=5.79]                       \n",
      "5it [00:00, 17.70it/s, epoch=50, train_acc=69, train_loss=5.16]                       \n",
      "5it [00:00, 17.42it/s, epoch=51, train_acc=74, train_loss=4.93]                       \n",
      "80it [00:15,  5.04it/s, test_acc=47.5, test_loss=0.0122]                        \n",
      "Saving...\n",
      "5it [00:00, 16.06it/s, epoch=52, train_acc=63, train_loss=5.26]                       \n",
      "5it [00:00, 17.68it/s, epoch=53, train_acc=62, train_loss=5.12]                       \n",
      "5it [00:00, 17.58it/s, epoch=54, train_acc=69, train_loss=4.19]                       \n",
      "5it [00:00, 17.42it/s, epoch=55, train_acc=60, train_loss=5.31]                       \n",
      "5it [00:00, 17.40it/s, epoch=56, train_acc=62, train_loss=4.83]                       \n",
      "5it [00:00, 17.61it/s, epoch=57, train_acc=64, train_loss=5.28]                       \n",
      "5it [00:00, 17.16it/s, epoch=58, train_acc=64, train_loss=4.75]                       \n",
      "5it [00:00, 16.54it/s, epoch=59, train_acc=63, train_loss=5.06]                       \n",
      "5it [00:00, 17.50it/s, epoch=60, train_acc=71, train_loss=3.9]                       \n",
      "5it [00:00, 17.70it/s, epoch=61, train_acc=66, train_loss=4.99]                       \n",
      "80it [00:15,  5.14it/s, test_acc=49.6, test_loss=0.0117]                        \n",
      "Saving...\n",
      "5it [00:00, 16.61it/s, epoch=62, train_acc=75, train_loss=4.47]                       \n",
      "5it [00:00, 17.75it/s, epoch=63, train_acc=68, train_loss=4.61]                       \n",
      "5it [00:00, 17.54it/s, epoch=64, train_acc=73, train_loss=4.82]                       \n",
      "5it [00:00, 17.66it/s, epoch=65, train_acc=77, train_loss=4.2]                       \n",
      "5it [00:00, 17.51it/s, epoch=66, train_acc=72, train_loss=4.58]                       \n",
      "5it [00:00, 17.76it/s, epoch=67, train_acc=71, train_loss=3.81]                       \n",
      "5it [00:00, 17.79it/s, epoch=68, train_acc=66, train_loss=4.46]                       \n",
      "5it [00:00, 17.54it/s, epoch=69, train_acc=67, train_loss=4.59]                       \n",
      "5it [00:00, 17.45it/s, epoch=70, train_acc=75, train_loss=4.12]                       \n",
      "5it [00:00, 17.75it/s, epoch=71, train_acc=72, train_loss=4.01]                       \n",
      "80it [00:15,  5.10it/s, test_acc=49.5, test_loss=0.0116]                        \n",
      "5it [00:00, 16.48it/s, epoch=72, train_acc=75, train_loss=3.67]                       \n",
      "5it [00:00, 17.44it/s, epoch=73, train_acc=69, train_loss=3.59]                       \n",
      "5it [00:00, 17.56it/s, epoch=74, train_acc=65, train_loss=4.41]                       \n",
      "5it [00:00, 17.60it/s, epoch=75, train_acc=69, train_loss=4.97]                       \n",
      "5it [00:00, 17.66it/s, epoch=76, train_acc=75, train_loss=4.35]                       \n",
      "5it [00:00, 17.40it/s, epoch=77, train_acc=69, train_loss=4.69]                       \n",
      "5it [00:00, 16.27it/s, epoch=78, train_acc=66, train_loss=3.96]                       \n",
      "5it [00:00, 17.40it/s, epoch=79, train_acc=70, train_loss=3.98]                       \n",
      "5it [00:00, 17.59it/s, epoch=80, train_acc=75, train_loss=4.44]                       \n",
      "5it [00:00, 15.31it/s, epoch=81, train_acc=71, train_loss=5]                       \n",
      "80it [00:16,  4.99it/s, test_acc=48.4, test_loss=0.0118]                        \n",
      "5it [00:00, 13.95it/s, epoch=82, train_acc=69, train_loss=3.93]                       \n",
      "5it [00:00, 15.06it/s, epoch=83, train_acc=73, train_loss=3.37]                       \n",
      "5it [00:00, 15.45it/s, epoch=84, train_acc=71, train_loss=3.89]                       \n",
      "5it [00:00, 15.49it/s, epoch=85, train_acc=72, train_loss=3.98]                       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 15.33it/s, epoch=86, train_acc=70, train_loss=4.26]                       \n",
      "5it [00:00, 15.46it/s, epoch=87, train_acc=72, train_loss=4.26]                       \n",
      "5it [00:00, 15.66it/s, epoch=88, train_acc=71, train_loss=3.72]                       \n",
      "5it [00:00, 15.31it/s, epoch=89, train_acc=72, train_loss=4.01]                       \n",
      "5it [00:00, 15.30it/s, epoch=90, train_acc=76, train_loss=4.27]                       \n",
      "5it [00:00, 17.96it/s, epoch=91, train_acc=74, train_loss=3.85]                       \n",
      "80it [00:16,  4.96it/s, test_acc=50.4, test_loss=0.0115]                        \n",
      "Saving...\n",
      "5it [00:00, 16.17it/s, epoch=92, train_acc=71, train_loss=4.18]                       \n",
      "5it [00:00, 17.50it/s, epoch=93, train_acc=72, train_loss=3.33]                       \n",
      "5it [00:00, 16.01it/s, epoch=94, train_acc=74, train_loss=4.09]                       \n",
      "5it [00:00, 17.86it/s, epoch=95, train_acc=72, train_loss=3.53]                       \n",
      "5it [00:00, 17.90it/s, epoch=96, train_acc=74, train_loss=3.21]                       \n",
      "5it [00:00, 16.42it/s, epoch=97, train_acc=81, train_loss=3.53]                       \n",
      "5it [00:00, 17.42it/s, epoch=98, train_acc=77, train_loss=3.33]                       \n",
      "5it [00:00, 15.77it/s, epoch=99, train_acc=76, train_loss=3.38]                       \n",
      "5it [00:00, 17.23it/s, epoch=100, train_acc=80, train_loss=3.97]                       \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    train_epoch(epoch, model, trainloader, optimizer, criterion)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        acc = val_epoch(model, testloader)\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            update_model(model, best_acc, epoch, \"pretrained_resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 16.76it/s, test_acc=77, test_loss=0.0423]                       \n",
      "80it [00:16,  4.94it/s, test_acc=50.4, test_loss=0.0115]                        \n",
      "470it [01:33,  5.03it/s, test_acc=50.6, test_loss=0.0113]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:  77.0\n",
      "Val   Acc:  50.41\n",
      "Cifar Acc:  50.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoint/pretrained_resnet18.t7\")['net'])\n",
    "\n",
    "train_acc = val_epoch(model, trainloader)\n",
    "val_acc = val_epoch(model, testloader)\n",
    "cifar_acc = val_epoch(model, cifarloader)\n",
    "\n",
    "print(\"Train Acc: \", train_acc)\n",
    "print(\"Val   Acc: \", val_acc)\n",
    "print(\"Cifar Acc: \", cifar_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on: https://pytorch.org/docs/stable/torchvision/models.html.\n",
    "\n",
    "__Question 5:__ Pick a model from the list above, adapt it for CIFAR and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We test the model on 3 datasets: the train dataset, the test dataset, and the results on shown as below:\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on Full Data | Reference github|\n",
    "|------|------|------|------|------|------|\n",
    "|   Pretrained ResNet18  | 100 | 77.0 | 50.4 | 50.61 | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGan features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs correspond to an unsupervised technique for generating images. In https://arxiv.org/pdf/1511.06434.pdf, Sec. 5.1 shows that the representation obtained from the Discriminator has some nice generalization properties on CIFAR10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    def __init__(self, nc=3, ndf=64):\n",
    "        super(_netD, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 2, 2, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def get_blocks(self, indices):\n",
    "        \n",
    "        self.blocks = []\n",
    "        \n",
    "        for index in indices:\n",
    "            self.blocks.append(list(self.children())[0][:index])\n",
    "            \n",
    "        self.max_layers = [nn.MaxPool2d(8, stride = 8),\n",
    "                           nn.MaxPool2d(4, stride = 4),\n",
    "                           nn.MaxPool2d(2, stride = 2)]\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.shape[0]\n",
    "        output1 = self.max_layers[0](self.blocks[0](input)).view(batch_size, -1)\n",
    "        output2 = self.max_layers[1](self.blocks[1](input)).view(batch_size, -1)\n",
    "        output3 = self.max_layers[2](self.blocks[2](input)).view(batch_size, -1)\n",
    "        output4 = self.blocks[3](input).view(batch_size, -1)\n",
    "        \n",
    "        output = torch.cat([output1, output2, output3, output4], dim = 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netD(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of colours\n",
    "NC = 3\n",
    "# Number Des filter\n",
    "NDF = 64\n",
    "\n",
    "netD = _netD(NC, NDF)\n",
    "netD.load_state_dict(torch.load(\"netD_epoch_199.pth\"))\n",
    "netD.get_blocks([2, 5, 8, 11])\n",
    "netD.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(\"../data\", train = 1, max_num = 100, transform = transform_test)\n",
    "testset = CIFAR10(\"../data\", train = 0, transform = transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True, num_workers = 1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader):\n",
    "    \n",
    "    X_features, X_targets = [], []\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        X_features.append(outputs.data.cpu().numpy())\n",
    "        X_targets.append(targets.data.cpu().numpy())\n",
    "        \n",
    "    return np.vstack(X_features), np.hstack(X_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_targets = extract_features(netD, trainloader)\n",
    "test_features, test_targets = extract_features(netD, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C = 10, gamma='scale')\n",
    "clf.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n",
      "Test  accuracy:  0.277\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy: \", clf.score(train_features, train_targets))\n",
    "print(\"Test  accuracy: \", clf.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6:__  Using for instance a pretrained model from https://github.com/soumith/dcgan.torch combined with https://github.com/pytorch/examples/tree/master/dcgan, propose a model to train on $\\mathcal{X}_{\\text{train}}$. Train it and report its accuracy.\n",
    "\n",
    "*Hint:* You can use the library: https://github.com/bshillingford/python-torchfile to load the weights of a model from torch(Lua) to pytorch(python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By using a pretrained discriminator of DCGAN, we extracted for each image a feature vector of dimension 15360. We trained a SVM on the first 100 images of CIFAR10 and the accuracies on train dataset and test dataset are reported as below:\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy | Reference github|\n",
    "|------|------|------|------|------|\n",
    "|   Pretrained DCGAN + SVM  | - | 1.0 | 27.7 | https://github.com/csinva/pytorch_gan_pretrained |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating *a priori*\n",
    "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that:\n",
    "\n",
    "$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n",
    "\n",
    "For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to:\n",
    "\n",
    "$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n",
    "\n",
    "Otherwise, one has to handle several boundary effects.\n",
    "\n",
    "__Question 7:__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the images are small, any operator may lead to a large distortion and then the image does no longer belong to the corresponding class. For example, if we rotate the image by 90 degrees, half of the pixels are missing thus we get an image with 256 pixel values, which is far from enough to provide enough information.\n",
    "\n",
    "> To overcome this issue, we can try following methods:\n",
    "> * Define a range of distortion for each operation\n",
    "> * Process the image in frequency domain\n",
    "> * Reject training examples with low image quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(36),\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(\"../data\", train = 1, max_num = 100, transform = transform_train)\n",
    "testset = CIFAR10(\"../data\", train = 0, transform = transform_test)\n",
    "cifarset = CIFAR10(\"../data\", train = -1, max_num = 60000, transform = transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 10, shuffle = True, num_workers = 1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = False, num_workers = 1)\n",
    "cifarloader = torch.utils.data.DataLoader(cifarset, batch_size = 64, shuffle = False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 34.32it/s, epoch=1, train_acc=13, train_loss=23.7]                        \n",
      "158it [00:03, 50.99it/s, test_acc=9.92, test_loss=0.0395]                         \n",
      "Saving...\n",
      "11it [00:00, 35.37it/s, epoch=2, train_acc=16, train_loss=23.5]                        \n",
      "11it [00:00, 38.46it/s, epoch=3, train_acc=19, train_loss=23.5]                        \n",
      "11it [00:00, 38.92it/s, epoch=4, train_acc=16, train_loss=23.2]                        \n",
      "11it [00:00, 38.15it/s, epoch=5, train_acc=23, train_loss=21.6]                        \n",
      "11it [00:00, 38.95it/s, epoch=6, train_acc=16, train_loss=22.8]                        \n",
      "11it [00:00, 38.90it/s, epoch=7, train_acc=17, train_loss=22.4]                        \n",
      "11it [00:00, 38.89it/s, epoch=8, train_acc=24, train_loss=21.6]                        \n",
      "11it [00:00, 37.99it/s, epoch=9, train_acc=18, train_loss=22.6]                        \n",
      "11it [00:00, 38.63it/s, epoch=10, train_acc=16, train_loss=21.5]                        \n",
      "11it [00:00, 39.01it/s, epoch=11, train_acc=24, train_loss=21.4]                        \n",
      "158it [00:03, 51.55it/s, test_acc=19.7, test_loss=0.0352]                         \n",
      "Saving...\n",
      "11it [00:00, 35.68it/s, epoch=12, train_acc=30, train_loss=21.4]                        \n",
      "11it [00:00, 37.02it/s, epoch=13, train_acc=18, train_loss=23]                        \n",
      "11it [00:00, 38.64it/s, epoch=14, train_acc=18, train_loss=21.1]                        \n",
      "11it [00:00, 35.99it/s, epoch=15, train_acc=24, train_loss=22.1]                        \n",
      "11it [00:00, 35.64it/s, epoch=16, train_acc=32, train_loss=20.9]                        \n",
      "11it [00:00, 38.75it/s, epoch=17, train_acc=26, train_loss=20.6]                        \n",
      "11it [00:00, 38.30it/s, epoch=18, train_acc=25, train_loss=20.6]                        \n",
      "11it [00:00, 38.93it/s, epoch=19, train_acc=30, train_loss=19.6]                        \n",
      "11it [00:00, 38.96it/s, epoch=20, train_acc=31, train_loss=19.5]                        \n",
      "11it [00:00, 38.54it/s, epoch=21, train_acc=36, train_loss=18.5]                        \n",
      "158it [00:03, 51.42it/s, test_acc=21.8, test_loss=0.0373]                         \n",
      "Saving...\n",
      "11it [00:00, 36.38it/s, epoch=22, train_acc=29, train_loss=20.3]                        \n",
      "11it [00:00, 37.49it/s, epoch=23, train_acc=32, train_loss=19.9]                        \n",
      "11it [00:00, 38.36it/s, epoch=24, train_acc=32, train_loss=20]                        \n",
      "11it [00:00, 34.51it/s, epoch=25, train_acc=29, train_loss=19.6]                        \n",
      "11it [00:00, 38.08it/s, epoch=26, train_acc=37, train_loss=18.7]                        \n",
      "11it [00:00, 37.46it/s, epoch=27, train_acc=37, train_loss=18.8]                        \n",
      "11it [00:00, 38.02it/s, epoch=28, train_acc=25, train_loss=18.7]                        \n",
      "11it [00:00, 38.32it/s, epoch=29, train_acc=37, train_loss=19.2]                        \n",
      "11it [00:00, 38.31it/s, epoch=30, train_acc=31, train_loss=18.9]                        \n",
      "11it [00:00, 39.15it/s, epoch=31, train_acc=36, train_loss=18.9]                        \n",
      "158it [00:03, 52.20it/s, test_acc=21.4, test_loss=0.0374]                         \n",
      "11it [00:00, 36.32it/s, epoch=32, train_acc=44, train_loss=17.3]                        \n",
      "11it [00:00, 39.09it/s, epoch=33, train_acc=38, train_loss=17.7]                        \n",
      "11it [00:00, 38.92it/s, epoch=34, train_acc=34, train_loss=17.6]                        \n",
      "11it [00:00, 39.37it/s, epoch=35, train_acc=41, train_loss=17.1]                        \n",
      "11it [00:00, 37.95it/s, epoch=36, train_acc=40, train_loss=16.8]                        \n",
      "11it [00:00, 35.36it/s, epoch=37, train_acc=38, train_loss=18.2]                        \n",
      "11it [00:00, 38.81it/s, epoch=38, train_acc=39, train_loss=18]                        \n",
      "11it [00:00, 38.62it/s, epoch=39, train_acc=37, train_loss=18.4]                        \n",
      "11it [00:00, 37.06it/s, epoch=40, train_acc=36, train_loss=17.8]                        \n",
      "11it [00:00, 38.88it/s, epoch=41, train_acc=45, train_loss=15.9]                        \n",
      "158it [00:03, 48.75it/s, test_acc=24.2, test_loss=0.0361]                         \n",
      "Saving...\n",
      "11it [00:00, 36.52it/s, epoch=42, train_acc=43, train_loss=16.6]                        \n",
      "11it [00:00, 38.67it/s, epoch=43, train_acc=36, train_loss=17.2]                        \n",
      "11it [00:00, 38.65it/s, epoch=44, train_acc=42, train_loss=16.3]                        \n",
      "11it [00:00, 38.31it/s, epoch=45, train_acc=35, train_loss=17.7]                        \n",
      "11it [00:00, 39.11it/s, epoch=46, train_acc=43, train_loss=15.8]                        \n",
      "11it [00:00, 33.63it/s, epoch=47, train_acc=48, train_loss=15.4]                        \n",
      "11it [00:00, 38.91it/s, epoch=48, train_acc=45, train_loss=16.4]                        \n",
      "11it [00:00, 38.37it/s, epoch=49, train_acc=45, train_loss=14.9]                        \n",
      "11it [00:00, 37.49it/s, epoch=50, train_acc=46, train_loss=15.4]                        \n",
      "11it [00:00, 38.20it/s, epoch=51, train_acc=36, train_loss=18]                        \n",
      "158it [00:03, 48.25it/s, test_acc=23.2, test_loss=0.0392]                         \n",
      "11it [00:00, 35.34it/s, epoch=52, train_acc=43, train_loss=16.8]                        \n",
      "11it [00:00, 38.98it/s, epoch=53, train_acc=50, train_loss=15.4]                        \n",
      "11it [00:00, 39.19it/s, epoch=54, train_acc=48, train_loss=14.3]                        \n",
      "11it [00:00, 36.02it/s, epoch=55, train_acc=46, train_loss=14.6]                        \n",
      "11it [00:00, 36.86it/s, epoch=56, train_acc=48, train_loss=14.3]                        \n",
      "11it [00:00, 39.10it/s, epoch=57, train_acc=48, train_loss=15.5]                        \n",
      "11it [00:00, 38.77it/s, epoch=58, train_acc=40, train_loss=17.6]                        \n",
      "11it [00:00, 37.94it/s, epoch=59, train_acc=52, train_loss=14.7]                        \n",
      "11it [00:00, 39.05it/s, epoch=60, train_acc=41, train_loss=14.4]                        \n",
      "11it [00:00, 38.50it/s, epoch=61, train_acc=49, train_loss=15.3]                        \n",
      "158it [00:03, 52.21it/s, test_acc=22.5, test_loss=0.0412]                         \n",
      "11it [00:00, 35.97it/s, epoch=62, train_acc=53, train_loss=14.3]                        \n",
      "11it [00:00, 38.33it/s, epoch=63, train_acc=53, train_loss=14.1]                        \n",
      "11it [00:00, 37.45it/s, epoch=64, train_acc=53, train_loss=13]                        \n",
      "11it [00:00, 36.16it/s, epoch=65, train_acc=52, train_loss=13.2]                        \n",
      "11it [00:00, 26.76it/s, epoch=66, train_acc=44, train_loss=16]                        \n",
      "11it [00:00, 38.82it/s, epoch=67, train_acc=54, train_loss=12.8]                        \n",
      "11it [00:00, 38.87it/s, epoch=68, train_acc=53, train_loss=13.9]                        \n",
      "11it [00:00, 38.12it/s, epoch=69, train_acc=48, train_loss=14.1]                        \n",
      "11it [00:00, 38.08it/s, epoch=70, train_acc=53, train_loss=12.2]                        \n",
      "11it [00:00, 38.50it/s, epoch=71, train_acc=52, train_loss=13.2]                        \n",
      "158it [00:03, 51.83it/s, test_acc=25.5, test_loss=0.0427]                         \n",
      "Saving...\n",
      "11it [00:00, 36.22it/s, epoch=72, train_acc=49, train_loss=14.6]                        \n",
      "11it [00:00, 38.11it/s, epoch=73, train_acc=52, train_loss=13.6]                        \n",
      "11it [00:00, 38.02it/s, epoch=74, train_acc=56, train_loss=13]                        \n",
      "11it [00:00, 38.67it/s, epoch=75, train_acc=56, train_loss=11.6]                        \n",
      "11it [00:00, 26.96it/s, epoch=76, train_acc=68, train_loss=9.45]                        \n",
      "11it [00:00, 38.05it/s, epoch=77, train_acc=56, train_loss=12.8]                        \n",
      "11it [00:00, 39.35it/s, epoch=78, train_acc=57, train_loss=13.9]                        \n",
      "11it [00:00, 36.72it/s, epoch=79, train_acc=54, train_loss=13.4]                        \n",
      "11it [00:00, 36.62it/s, epoch=80, train_acc=63, train_loss=11.3]                        \n",
      "11it [00:00, 36.39it/s, epoch=81, train_acc=61, train_loss=11.9]                        \n",
      "158it [00:03, 51.30it/s, test_acc=24.1, test_loss=0.0457]                         \n",
      "11it [00:00, 35.83it/s, epoch=82, train_acc=60, train_loss=12.3]                        \n",
      "11it [00:00, 38.63it/s, epoch=83, train_acc=61, train_loss=11.7]                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 38.78it/s, epoch=84, train_acc=58, train_loss=11.3]                        \n",
      "11it [00:00, 39.08it/s, epoch=85, train_acc=60, train_loss=10.8]                        \n",
      "11it [00:00, 38.96it/s, epoch=86, train_acc=58, train_loss=10.6]                        \n",
      "11it [00:00, 36.77it/s, epoch=87, train_acc=56, train_loss=11.8]                        \n",
      "11it [00:00, 38.47it/s, epoch=88, train_acc=63, train_loss=10.2]                        \n",
      "11it [00:00, 38.90it/s, epoch=89, train_acc=63, train_loss=11.4]                        \n",
      "11it [00:00, 38.34it/s, epoch=90, train_acc=60, train_loss=12.5]                        \n",
      "11it [00:00, 38.05it/s, epoch=91, train_acc=57, train_loss=10.6]                        \n",
      "158it [00:03, 50.53it/s, test_acc=25.1, test_loss=0.0466]                         \n",
      "11it [00:00, 34.72it/s, epoch=92, train_acc=55, train_loss=11.7]                        \n",
      "11it [00:00, 26.12it/s, epoch=93, train_acc=72, train_loss=8.79]                        \n",
      "11it [00:00, 37.53it/s, epoch=94, train_acc=56, train_loss=11.2]                        \n",
      "11it [00:00, 34.62it/s, epoch=95, train_acc=67, train_loss=10]                        \n",
      "11it [00:00, 36.56it/s, epoch=96, train_acc=65, train_loss=10.1]                        \n",
      "11it [00:00, 37.76it/s, epoch=97, train_acc=63, train_loss=10.8]                        \n",
      "11it [00:00, 36.76it/s, epoch=98, train_acc=61, train_loss=10.8]                        \n",
      "11it [00:00, 35.96it/s, epoch=99, train_acc=64, train_loss=9.12]                        \n",
      "11it [00:00, 26.58it/s, epoch=100, train_acc=65, train_loss=9.46]                        \n"
     ]
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 5e-4)\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    train_epoch(epoch, net, trainloader, optimizer, criterion)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        acc = val_epoch(net, testloader)\n",
    "        \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        update_model(net, best_acc, epoch, \"resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 63.41it/s, test_acc=43, test_loss=0.143]                        \n",
      "158it [00:03, 49.86it/s, test_acc=25.5, test_loss=0.0427]                         \n",
      "939it [00:17, 52.50it/s, test_acc=25.5, test_loss=0.0425]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:  43.0\n",
      "Val   Acc:  25.52\n",
      "Cifar Acc:  25.541666666666668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"checkpoint/resnet18.t7\")['net'])\n",
    "\n",
    "train_acc = val_epoch(net, trainloader)\n",
    "val_acc = val_epoch(net, testloader)\n",
    "cifar_acc = val_epoch(net, cifarloader)\n",
    "\n",
    "print(\"Train Acc: \", train_acc)\n",
    "print(\"Val   Acc: \", val_acc)\n",
    "print(\"Cifar Acc: \", cifar_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 43.92it/s, epoch=1, train_acc=6, train_loss=24.6]                        \n",
      "158it [00:01, 86.06it/s, test_acc=12.7, test_loss=0.0392]                         \n",
      "Saving...\n",
      "11it [00:00, 32.74it/s, epoch=2, train_acc=12, train_loss=23.6]                        \n",
      "11it [00:00, 35.03it/s, epoch=3, train_acc=22, train_loss=24.5]                        \n",
      "11it [00:00, 43.31it/s, epoch=4, train_acc=11, train_loss=27.4]                        \n",
      "11it [00:00, 42.87it/s, epoch=5, train_acc=22, train_loss=22.9]                        \n",
      "11it [00:00, 42.96it/s, epoch=6, train_acc=16, train_loss=22.7]                        \n",
      "11it [00:00, 43.02it/s, epoch=7, train_acc=20, train_loss=23.4]                        \n",
      "11it [00:00, 42.38it/s, epoch=8, train_acc=24, train_loss=22.5]                        \n",
      "11it [00:00, 43.16it/s, epoch=9, train_acc=22, train_loss=21.9]                        \n",
      "11it [00:00, 43.40it/s, epoch=10, train_acc=25, train_loss=20.9]                        \n",
      "11it [00:00, 43.06it/s, epoch=11, train_acc=23, train_loss=22.5]                        \n",
      "158it [00:01, 85.63it/s, test_acc=20.4, test_loss=0.0369]                         \n",
      "Saving...\n",
      "11it [00:00, 33.11it/s, epoch=12, train_acc=24, train_loss=20.6]                        \n",
      "11it [00:00, 43.30it/s, epoch=13, train_acc=28, train_loss=20.6]                        \n",
      "11it [00:00, 43.50it/s, epoch=14, train_acc=28, train_loss=20.1]                        \n",
      "11it [00:00, 43.98it/s, epoch=15, train_acc=27, train_loss=20.9]                        \n",
      "11it [00:00, 43.78it/s, epoch=16, train_acc=27, train_loss=19.1]                        \n",
      "11it [00:00, 43.23it/s, epoch=17, train_acc=31, train_loss=20.4]                        \n",
      "11it [00:00, 42.76it/s, epoch=18, train_acc=26, train_loss=20.9]                        \n",
      "11it [00:00, 43.28it/s, epoch=19, train_acc=31, train_loss=18.8]                        \n",
      "11it [00:00, 43.67it/s, epoch=20, train_acc=31, train_loss=19.4]                        \n",
      "11it [00:00, 35.58it/s, epoch=21, train_acc=31, train_loss=18.9]                        \n",
      "158it [00:01, 86.31it/s, test_acc=22.7, test_loss=0.0368]                         \n",
      "Saving...\n",
      "11it [00:00, 24.31it/s, epoch=22, train_acc=40, train_loss=18.2]                        \n",
      "11it [00:00, 43.63it/s, epoch=23, train_acc=46, train_loss=17.3]                        \n",
      "11it [00:00, 41.42it/s, epoch=24, train_acc=39, train_loss=17.7]                        \n",
      "11it [00:00, 32.78it/s, epoch=25, train_acc=35, train_loss=17.4]                        \n",
      "11it [00:00, 43.91it/s, epoch=26, train_acc=42, train_loss=16.6]                        \n",
      "11it [00:00, 42.62it/s, epoch=27, train_acc=39, train_loss=17.5]                        \n",
      "11it [00:00, 43.63it/s, epoch=28, train_acc=43, train_loss=16.6]                        \n",
      "11it [00:00, 42.29it/s, epoch=29, train_acc=48, train_loss=16.3]                        \n",
      "11it [00:00, 42.44it/s, epoch=30, train_acc=43, train_loss=16.1]                        \n",
      "11it [00:00, 42.36it/s, epoch=31, train_acc=39, train_loss=18.8]                        \n",
      "158it [00:01, 85.03it/s, test_acc=22.6, test_loss=0.0386]                         \n",
      "11it [00:00, 24.15it/s, epoch=32, train_acc=44, train_loss=17.6]                        \n",
      "11it [00:00, 43.61it/s, epoch=33, train_acc=40, train_loss=17.8]                        \n",
      "11it [00:00, 43.64it/s, epoch=34, train_acc=46, train_loss=17.2]                        \n",
      "11it [00:00, 43.59it/s, epoch=35, train_acc=50, train_loss=14.1]                        \n",
      "11it [00:00, 42.64it/s, epoch=36, train_acc=40, train_loss=15.6]                        \n",
      "11it [00:00, 42.86it/s, epoch=37, train_acc=48, train_loss=15.1]                        \n",
      "11it [00:00, 42.69it/s, epoch=38, train_acc=49, train_loss=14.2]                        \n",
      "11it [00:00, 43.61it/s, epoch=39, train_acc=46, train_loss=15.2]                        \n",
      "11it [00:00, 43.40it/s, epoch=40, train_acc=40, train_loss=17]                        \n",
      "11it [00:00, 43.74it/s, epoch=41, train_acc=54, train_loss=12.6]                        \n",
      "158it [00:01, 83.83it/s, test_acc=21.2, test_loss=0.0409]                         \n",
      "11it [00:00, 23.74it/s, epoch=42, train_acc=46, train_loss=14.4]                        \n",
      "11it [00:00, 43.59it/s, epoch=43, train_acc=52, train_loss=13.6]                        \n",
      "11it [00:00, 43.36it/s, epoch=44, train_acc=48, train_loss=14.2]                        \n",
      "11it [00:00, 34.08it/s, epoch=45, train_acc=63, train_loss=12.4]                        \n",
      "11it [00:00, 42.69it/s, epoch=46, train_acc=51, train_loss=14.3]                        \n",
      "11it [00:00, 43.28it/s, epoch=47, train_acc=59, train_loss=13.1]                        \n",
      "11it [00:00, 43.81it/s, epoch=48, train_acc=50, train_loss=14.2]                        \n",
      "11it [00:00, 43.25it/s, epoch=49, train_acc=50, train_loss=14.9]                        \n",
      "11it [00:00, 43.97it/s, epoch=50, train_acc=60, train_loss=12.6]                        \n",
      "11it [00:00, 43.74it/s, epoch=51, train_acc=47, train_loss=13.3]                        \n",
      "158it [00:01, 83.76it/s, test_acc=24.2, test_loss=0.0407]                         \n",
      "Saving...\n",
      "11it [00:00, 24.47it/s, epoch=52, train_acc=55, train_loss=11.7]                        \n",
      "11it [00:00, 43.17it/s, epoch=53, train_acc=57, train_loss=12.1]                        \n",
      "11it [00:00, 42.77it/s, epoch=54, train_acc=55, train_loss=12.8]                        \n",
      "11it [00:00, 42.04it/s, epoch=55, train_acc=52, train_loss=13.3]                        \n",
      "11it [00:00, 43.20it/s, epoch=56, train_acc=59, train_loss=12]                        \n",
      "11it [00:00, 43.48it/s, epoch=57, train_acc=61, train_loss=11.7]                        \n",
      "11it [00:00, 42.85it/s, epoch=58, train_acc=63, train_loss=10.7]                        \n",
      "11it [00:00, 34.64it/s, epoch=59, train_acc=68, train_loss=9.7]                        \n",
      "11it [00:00, 42.98it/s, epoch=60, train_acc=73, train_loss=8.9]                        \n",
      "11it [00:00, 42.83it/s, epoch=61, train_acc=69, train_loss=10.4]                        \n",
      "158it [00:01, 86.01it/s, test_acc=24.1, test_loss=0.0472]                         \n",
      "11it [00:00, 25.08it/s, epoch=62, train_acc=71, train_loss=8.97]                        \n",
      "11it [00:00, 43.57it/s, epoch=63, train_acc=67, train_loss=10.5]                        \n",
      "11it [00:00, 42.20it/s, epoch=64, train_acc=60, train_loss=10.9]                        \n",
      "11it [00:00, 42.96it/s, epoch=65, train_acc=63, train_loss=11.7]                        \n",
      "11it [00:00, 42.74it/s, epoch=66, train_acc=62, train_loss=9.81]                        \n",
      "11it [00:00, 43.62it/s, epoch=67, train_acc=66, train_loss=10.6]                        \n",
      "11it [00:00, 43.38it/s, epoch=68, train_acc=70, train_loss=9.35]                        \n",
      "11it [00:00, 43.45it/s, epoch=69, train_acc=72, train_loss=7.51]                        \n",
      "11it [00:00, 42.89it/s, epoch=70, train_acc=80, train_loss=5.96]                        \n",
      "11it [00:00, 35.27it/s, epoch=71, train_acc=73, train_loss=8.35]                        \n",
      "158it [00:01, 76.52it/s, test_acc=27.5, test_loss=0.0483]                         \n",
      "Saving...\n",
      "11it [00:00, 24.75it/s, epoch=72, train_acc=74, train_loss=8.84]                        \n",
      "11it [00:00, 34.06it/s, epoch=73, train_acc=77, train_loss=6.37]                        \n",
      "11it [00:00, 34.30it/s, epoch=74, train_acc=69, train_loss=8.36]                        \n",
      "11it [00:00, 41.97it/s, epoch=75, train_acc=72, train_loss=9.06]                        \n",
      "11it [00:00, 42.93it/s, epoch=76, train_acc=67, train_loss=9.25]                        \n",
      "11it [00:00, 34.79it/s, epoch=77, train_acc=75, train_loss=7.1]                        \n",
      "11it [00:00, 43.02it/s, epoch=78, train_acc=76, train_loss=7.13]                        \n",
      "11it [00:00, 42.75it/s, epoch=79, train_acc=73, train_loss=7.26]                        \n",
      "11it [00:00, 42.14it/s, epoch=80, train_acc=79, train_loss=6.76]                        \n",
      "11it [00:00, 42.39it/s, epoch=81, train_acc=74, train_loss=8.2]                        \n",
      "158it [00:01, 82.66it/s, test_acc=26.6, test_loss=0.0524]                         \n",
      "11it [00:00, 32.81it/s, epoch=82, train_acc=81, train_loss=5.31]                        \n",
      "11it [00:00, 42.40it/s, epoch=83, train_acc=68, train_loss=8.7]                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 33.84it/s, epoch=84, train_acc=83, train_loss=6.15]                        \n",
      "11it [00:00, 43.28it/s, epoch=85, train_acc=71, train_loss=6.65]                        \n",
      "11it [00:00, 43.16it/s, epoch=86, train_acc=73, train_loss=9.53]                        \n",
      "11it [00:00, 43.03it/s, epoch=87, train_acc=69, train_loss=8.91]                        \n",
      "11it [00:00, 42.69it/s, epoch=88, train_acc=68, train_loss=8.4]                        \n",
      "11it [00:00, 44.05it/s, epoch=89, train_acc=75, train_loss=7.29]                        \n",
      "11it [00:00, 35.37it/s, epoch=90, train_acc=76, train_loss=6.66]                        \n",
      "11it [00:00, 43.19it/s, epoch=91, train_acc=72, train_loss=6.89]                        \n",
      "158it [00:01, 84.62it/s, test_acc=24.8, test_loss=0.0567]                         \n",
      "11it [00:00, 24.65it/s, epoch=92, train_acc=76, train_loss=7.07]                        \n",
      "11it [00:00, 43.37it/s, epoch=93, train_acc=75, train_loss=6.69]                        \n",
      "11it [00:00, 43.73it/s, epoch=94, train_acc=82, train_loss=5.98]                        \n",
      "11it [00:00, 43.54it/s, epoch=95, train_acc=79, train_loss=5.35]                        \n",
      "11it [00:00, 44.05it/s, epoch=96, train_acc=71, train_loss=7.65]                        \n",
      "11it [00:00, 42.73it/s, epoch=97, train_acc=87, train_loss=4.16]                        \n",
      "11it [00:00, 35.57it/s, epoch=98, train_acc=78, train_loss=6.04]                        \n",
      "11it [00:00, 41.63it/s, epoch=99, train_acc=78, train_loss=6.91]                        \n",
      "11it [00:00, 41.99it/s, epoch=100, train_acc=84, train_loss=6.32]                        \n"
     ]
    }
   ],
   "source": [
    "net = VGG(\"VGG11\")\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.005, momentum = 0.9, weight_decay = 5e-4)\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    train_epoch(epoch, net, trainloader, optimizer, criterion)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        acc = val_epoch(net, testloader)\n",
    "        \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        update_model(net, best_acc, epoch, \"vgg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 61.63it/s, test_acc=79, test_loss=0.0586]                        \n",
      "158it [00:01, 83.08it/s, test_acc=27.5, test_loss=0.0483]                         \n",
      "939it [00:10, 88.36it/s, test_acc=27.3, test_loss=0.0481]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:  79.0\n",
      "Val   Acc:  27.49\n",
      "Cifar Acc:  27.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"checkpoint/vgg.t7\")['net'])\n",
    "\n",
    "train_acc = val_epoch(net, trainloader)\n",
    "val_acc = val_epoch(net, testloader)\n",
    "cifar_acc = val_epoch(net, cifarloader)\n",
    "\n",
    "print(\"Train Acc: \", train_acc)\n",
    "print(\"Val   Acc: \", val_acc)\n",
    "print(\"Cifar Acc: \", cifar_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8:__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ and __Question 4__ with them and report the accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> We use following transformation methods: horizental flip, rotation, random crop, rescaling, color adjusting. Then the models in Q3 and Q4 are retrained.\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on Full Data | Reference github|\n",
    "|------|------|------|------|------|------|\n",
    "|   ResNet18  | 100 | 43.0 | 25.52 | 25.54 | https://github.com/kuangliu/pytorch-cifar |\n",
    "|   VGG11  | 100 | 79.0 | 27.49 | 27.26 | https://github.com/kuangliu/pytorch-cifar |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelets\n",
    "\n",
    "__Question 9:__ Use a Scattering Transform as an input to a ResNet-like architecture. You can find a baseline here: https://arxiv.org/pdf/1703.08961.pdf.\n",
    "\n",
    "*Hint:* You can use the following package: https://www.kymat.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kymatio import Scattering2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_transform(nn.Module):\n",
    "    def __init__(self, block, num_blocks, k, num_classes=10):\n",
    "        super(ResNet_transform, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.k = k\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(self.k)\n",
    "        self.conv1 = nn.Conv2d(self.k, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn0(x.view(-1, self.k, 8, 8))\n",
    "        out = F.relu(self.bn1(self.conv1(out)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18_transform(k):\n",
    "    return ResNet_transform(BasicBlock, [2, 2, 2, 2], k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_transform(epoch, net, trainloader, optimizer, criterion, scattering):\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    with trange(len(trainloader), file=sys.stderr) as t:\n",
    "        net.train()\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(scattering(inputs))\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            t.update()\n",
    "\n",
    "        t.set_postfix(train_loss = train_loss, train_acc = correct * 100. / total, epoch = epoch + 1)\n",
    "        t.update()\n",
    "        \n",
    "def val_epoch_transform(net, testloader, scattering):\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "        \n",
    "    with trange(len(testloader), file=sys.stderr) as t:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(scattering(inputs))\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                t.update()\n",
    "\n",
    "        t.set_postfix(test_loss = test_loss / total, test_acc = correct * 100. / total)\n",
    "        t.update(1)\n",
    "            \n",
    "    return correct * 100. / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(\"../data\", train = 1, max_num = 100, transform = transform_train)\n",
    "testset = CIFAR10(\"../data\", train = 0, transform = transform_test)\n",
    "cifarset = CIFAR10(\"../data\", train = -1, max_num = 60000, transform = transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 10, shuffle = True, num_workers = 1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = False, num_workers = 1)\n",
    "cifarloader = torch.utils.data.DataLoader(cifarset, batch_size = 64, shuffle = False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 11.60it/s, epoch=1, train_acc=13, train_loss=42.7]                        \n",
      "158it [00:11, 13.67it/s, test_acc=12.9, test_loss=0.0671]                         \n",
      "11it [00:00, 12.08it/s, epoch=2, train_acc=18, train_loss=64.3]                        \n",
      "11it [00:00, 12.06it/s, epoch=3, train_acc=6, train_loss=48.9]                        \n",
      "11it [00:00, 11.93it/s, epoch=4, train_acc=18, train_loss=29.4]                        \n",
      "11it [00:00, 11.88it/s, epoch=5, train_acc=12, train_loss=26.7]                        \n",
      "11it [00:00, 12.05it/s, epoch=6, train_acc=12, train_loss=23.4]                        \n",
      "11it [00:00, 11.98it/s, epoch=7, train_acc=20, train_loss=22.5]                        \n",
      "11it [00:00, 11.89it/s, epoch=8, train_acc=18, train_loss=21.3]                        \n",
      "11it [00:00, 11.99it/s, epoch=9, train_acc=26, train_loss=21.2]                        \n",
      "11it [00:00, 11.34it/s, epoch=10, train_acc=18, train_loss=21.6]                        \n",
      "11it [00:00, 11.14it/s, epoch=11, train_acc=12, train_loss=21.8]                        \n",
      "158it [00:12, 13.05it/s, test_acc=13.9, test_loss=0.0365]                         \n",
      "11it [00:00, 11.89it/s, epoch=12, train_acc=25, train_loss=20.6]                        \n",
      "11it [00:00, 12.03it/s, epoch=13, train_acc=26, train_loss=19.5]                        \n",
      "11it [00:00, 12.05it/s, epoch=14, train_acc=23, train_loss=20.9]                        \n",
      "11it [00:00, 11.88it/s, epoch=15, train_acc=27, train_loss=19.8]                        \n",
      "11it [00:00, 12.03it/s, epoch=16, train_acc=23, train_loss=19.9]                        \n",
      "11it [00:00, 12.03it/s, epoch=17, train_acc=30, train_loss=20.6]                        \n",
      "11it [00:00, 12.12it/s, epoch=18, train_acc=24, train_loss=19.5]                        \n",
      "11it [00:00, 11.66it/s, epoch=19, train_acc=18, train_loss=19.1]                        \n",
      "11it [00:00, 11.90it/s, epoch=20, train_acc=28, train_loss=19.4]                        \n",
      "11it [00:00, 11.97it/s, epoch=21, train_acc=29, train_loss=20.2]                        \n",
      "158it [00:11, 13.50it/s, test_acc=15.2, test_loss=0.0379]                         \n",
      "11it [00:00, 12.11it/s, epoch=22, train_acc=30, train_loss=18.7]                        \n",
      "11it [00:00, 12.11it/s, epoch=23, train_acc=20, train_loss=19.7]                        \n",
      "11it [00:00, 11.88it/s, epoch=24, train_acc=29, train_loss=18.2]                        \n",
      "11it [00:00, 12.01it/s, epoch=25, train_acc=28, train_loss=19.7]                        \n",
      "11it [00:00, 12.08it/s, epoch=26, train_acc=33, train_loss=18.1]                        \n",
      "11it [00:00, 12.08it/s, epoch=27, train_acc=31, train_loss=18.6]                        \n",
      "11it [00:00, 12.18it/s, epoch=28, train_acc=28, train_loss=18.6]                        \n",
      "11it [00:00, 12.14it/s, epoch=29, train_acc=30, train_loss=18.7]                        \n",
      "11it [00:00, 11.87it/s, epoch=30, train_acc=32, train_loss=17.4]                        \n",
      "11it [00:00, 12.10it/s, epoch=31, train_acc=30, train_loss=17.9]                        \n",
      "158it [00:11, 13.71it/s, test_acc=19.9, test_loss=0.0382]                         \n",
      "11it [00:00, 12.03it/s, epoch=32, train_acc=30, train_loss=18.4]                        \n",
      "11it [00:00, 12.12it/s, epoch=33, train_acc=38, train_loss=17.3]                        \n",
      "11it [00:00, 11.79it/s, epoch=34, train_acc=33, train_loss=18]                        \n",
      "11it [00:00, 11.83it/s, epoch=35, train_acc=33, train_loss=18.5]                        \n",
      "11it [00:00, 12.07it/s, epoch=36, train_acc=36, train_loss=17.6]                        \n",
      "11it [00:00, 11.67it/s, epoch=37, train_acc=33, train_loss=20]                        \n",
      "11it [00:00, 12.07it/s, epoch=38, train_acc=39, train_loss=17.1]                        \n",
      "11it [00:00, 12.00it/s, epoch=39, train_acc=36, train_loss=18.3]                        \n",
      "11it [00:00, 11.92it/s, epoch=40, train_acc=30, train_loss=18.4]                        \n",
      "11it [00:00, 11.90it/s, epoch=41, train_acc=39, train_loss=17.9]                        \n",
      "158it [00:11, 13.53it/s, test_acc=20.1, test_loss=0.0383]                         \n",
      "11it [00:00, 12.03it/s, epoch=42, train_acc=41, train_loss=17.2]                        \n",
      "11it [00:00, 12.06it/s, epoch=43, train_acc=39, train_loss=16.2]                        \n",
      "11it [00:00, 11.82it/s, epoch=44, train_acc=43, train_loss=16.6]                        \n",
      "11it [00:00, 12.17it/s, epoch=45, train_acc=39, train_loss=16.3]                        \n",
      "11it [00:00, 11.78it/s, epoch=46, train_acc=50, train_loss=15.6]                        \n",
      "11it [00:00, 11.99it/s, epoch=47, train_acc=52, train_loss=13.3]                        \n",
      "11it [00:00, 11.93it/s, epoch=48, train_acc=51, train_loss=13.6]                        \n",
      "11it [00:00, 12.07it/s, epoch=49, train_acc=47, train_loss=14.1]                        \n",
      "11it [00:00, 12.00it/s, epoch=50, train_acc=62, train_loss=12.1]                        \n",
      "11it [00:00, 12.11it/s, epoch=51, train_acc=59, train_loss=11.8]                        \n",
      "158it [00:11, 13.75it/s, test_acc=24.7, test_loss=0.0483]                         \n",
      "11it [00:00, 11.92it/s, epoch=52, train_acc=58, train_loss=12.5]                        \n",
      "11it [00:00, 11.92it/s, epoch=53, train_acc=59, train_loss=12.6]                        \n",
      "11it [00:00, 11.99it/s, epoch=54, train_acc=60, train_loss=12.4]                        \n",
      "11it [00:00, 12.01it/s, epoch=55, train_acc=60, train_loss=12.5]                        \n",
      "11it [00:00, 12.19it/s, epoch=56, train_acc=60, train_loss=11.6]                        \n",
      "11it [00:00, 11.90it/s, epoch=57, train_acc=68, train_loss=9.19]                        \n",
      "11it [00:00, 12.01it/s, epoch=58, train_acc=65, train_loss=10.6]                        \n",
      "11it [00:00, 11.97it/s, epoch=59, train_acc=70, train_loss=10.7]                        \n",
      "11it [00:00, 11.88it/s, epoch=60, train_acc=65, train_loss=9.38]                        \n",
      "11it [00:00, 12.00it/s, epoch=61, train_acc=67, train_loss=12.1]                        \n",
      "158it [00:11, 13.60it/s, test_acc=22.2, test_loss=0.0545]                         \n",
      "11it [00:00, 11.65it/s, epoch=62, train_acc=75, train_loss=9.25]                        \n",
      "11it [00:00, 11.86it/s, epoch=63, train_acc=74, train_loss=7.57]                        \n",
      "11it [00:00, 11.88it/s, epoch=64, train_acc=71, train_loss=7.89]                        \n",
      "11it [00:00, 12.05it/s, epoch=65, train_acc=76, train_loss=7.74]                        \n",
      "11it [00:00, 11.97it/s, epoch=66, train_acc=68, train_loss=10.8]                        \n",
      "11it [00:00, 11.76it/s, epoch=67, train_acc=78, train_loss=7.82]                        \n",
      "11it [00:00, 11.74it/s, epoch=68, train_acc=84, train_loss=5.94]                        \n",
      "11it [00:00, 11.43it/s, epoch=69, train_acc=81, train_loss=5.67]                        \n",
      "11it [00:00, 11.82it/s, epoch=70, train_acc=82, train_loss=5.88]                        \n",
      "11it [00:00, 11.92it/s, epoch=71, train_acc=79, train_loss=6.27]                        \n",
      "158it [00:11, 13.37it/s, test_acc=25.3, test_loss=0.0694]                         \n",
      "11it [00:00, 11.59it/s, epoch=72, train_acc=76, train_loss=8.44]                        \n",
      "11it [00:00, 11.91it/s, epoch=73, train_acc=75, train_loss=7.11]                        \n",
      "11it [00:00, 11.77it/s, epoch=74, train_acc=70, train_loss=9.21]                        \n",
      "11it [00:00, 11.98it/s, epoch=75, train_acc=70, train_loss=8.28]                        \n",
      "11it [00:00, 11.74it/s, epoch=76, train_acc=87, train_loss=3.86]                        \n",
      "11it [00:00, 11.93it/s, epoch=77, train_acc=81, train_loss=6.75]                        \n",
      "11it [00:00, 11.93it/s, epoch=78, train_acc=81, train_loss=7.26]                        \n",
      "11it [00:00, 11.91it/s, epoch=79, train_acc=75, train_loss=7.37]                        \n",
      "11it [00:00, 11.91it/s, epoch=80, train_acc=81, train_loss=7.44]                        \n",
      "11it [00:00, 11.97it/s, epoch=81, train_acc=80, train_loss=5.76]                        \n",
      "158it [00:11, 13.34it/s, test_acc=29.4, test_loss=0.0609]                         \n",
      "Saving...\n",
      "11it [00:00, 11.71it/s, epoch=82, train_acc=80, train_loss=6.46]                        \n",
      "11it [00:00, 11.55it/s, epoch=83, train_acc=76, train_loss=7.86]                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 11.80it/s, epoch=84, train_acc=82, train_loss=6.22]                        \n",
      "11it [00:00, 11.75it/s, epoch=85, train_acc=77, train_loss=7.02]                        \n",
      "11it [00:00, 12.05it/s, epoch=86, train_acc=78, train_loss=7.11]                        \n",
      "11it [00:00, 11.88it/s, epoch=87, train_acc=83, train_loss=5.27]                        \n",
      "11it [00:00, 11.87it/s, epoch=88, train_acc=87, train_loss=4.41]                        \n",
      "11it [00:00, 12.03it/s, epoch=89, train_acc=81, train_loss=5.77]                        \n",
      "11it [00:00, 12.04it/s, epoch=90, train_acc=90, train_loss=2.91]                        \n",
      "11it [00:00, 12.09it/s, epoch=91, train_acc=87, train_loss=2.66]                        \n",
      "158it [00:11, 13.57it/s, test_acc=27.7, test_loss=0.0782]                         \n",
      "11it [00:00, 11.61it/s, epoch=92, train_acc=88, train_loss=3.44]                        \n",
      "11it [00:00, 12.07it/s, epoch=93, train_acc=81, train_loss=6.98]                        \n",
      "11it [00:00, 11.43it/s, epoch=94, train_acc=89, train_loss=5.05]                        \n",
      "11it [00:00, 11.85it/s, epoch=95, train_acc=74, train_loss=8.34]                        \n",
      "11it [00:00, 11.82it/s, epoch=96, train_acc=82, train_loss=8.19]                        \n",
      "11it [00:00, 11.91it/s, epoch=97, train_acc=80, train_loss=6.04]                        \n",
      "11it [00:00, 11.50it/s, epoch=98, train_acc=80, train_loss=7.23]                        \n",
      "11it [00:00, 11.75it/s, epoch=99, train_acc=81, train_loss=6.26]                        \n",
      "11it [00:00, 11.66it/s, epoch=100, train_acc=87, train_loss=3.96]                        \n"
     ]
    }
   ],
   "source": [
    "scattering = Scattering2D(J = 2, shape = (32, 32))\n",
    "K = 81 * 3\n",
    "\n",
    "net = ResNet18_transform(K)\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.1, momentum = 0.9, weight_decay = 5e-4)\n",
    "\n",
    "if device == 'cuda':\n",
    "    scattering = scattering.cuda()\n",
    "    \n",
    "for epoch in range(100):\n",
    "    \n",
    "    train_epoch_transform(epoch, net, trainloader, optimizer, criterion, scattering)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        acc = val_epoch_transform(net, testloader, scattering)\n",
    "        \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        update_model(net, best_acc, epoch, \"resnet18_scattering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 13.14it/s, test_acc=89, test_loss=0.0433]                        \n",
      "158it [00:11, 13.74it/s, test_acc=29.4, test_loss=0.0609]                         \n",
      "939it [01:08, 14.86it/s, test_acc=29, test_loss=0.0608]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc:  89.0\n",
      "Val   Acc:  29.43\n",
      "Cifar Acc:  28.961666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"checkpoint/resnet18_scattering.t7\")['net'])\n",
    "\n",
    "train_acc = val_epoch_transform(net, trainloader, scattering)\n",
    "val_acc = val_epoch_transform(net, testloader, scattering)\n",
    "cifar_acc = val_epoch_transform(net, cifarloader, scattering)\n",
    "\n",
    "print(\"Train Acc: \", train_acc)\n",
    "print(\"Val   Acc: \", val_acc)\n",
    "print(\"Cifar Acc: \", cifar_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on Full Data | Reference github|\n",
    "|------|------|------|------|------|------|\n",
    "|  ResNet18 with Scaling  | 100 | 89 | 29.4 | 28.96 | https://www.kymat.io/gallery_2d/cifar.html#sphx-glr-gallery-2d-cifar-py |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weakly supervised techniques permit to tackle the issue of labeled data. An introduction to those techniques can be found here: https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, axis = 1):\n",
    "    m = torch.max(x, dim = 1)[0]\n",
    "    return m + torch.log(torch.sum(torch.exp(x - m.unsqueeze(1)), dim = axis))\n",
    "\n",
    "class LinearWeightNorm(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, weight_scale=None, weight_init_stdv=0.1):\n",
    "        super(LinearWeightNorm, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * weight_init_stdv)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        if weight_scale is not None:\n",
    "            assert type(weight_scale) == int\n",
    "            self.weight_scale = nn.Parameter(torch.ones(out_features, 1) * weight_scale)\n",
    "        else:\n",
    "            self.weight_scale = 1 \n",
    "    def forward(self, x):\n",
    "        W = self.weight * self.weight_scale / torch.sqrt(torch.sum(self.weight ** 2, dim = 1, keepdim = True))\n",
    "        return F.linear(x, W, self.bias)\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', weight_scale=' + str(self.weight_scale) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Structure ----- GAN based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedGAN(object):\n",
    "    \n",
    "    def __init__(self, G, D, labeled, unlabeled, test, savedir = 'checkpoint/', batchsize = 64, \n",
    "                 lr = 0.001, momentum = 0.5, log_interval = 500, unlabel_weight = 0.1, eval_interval = 10):\n",
    "        \n",
    "        self.G = G\n",
    "        self.D = D\n",
    "        torch.save(self.G, os.path.join(savedir, 'G.pkl'))\n",
    "        torch.save(self.D, os.path.join(savedir, 'D.pkl'))\n",
    "        \n",
    "        self.savedir = savedir\n",
    "        self.writer = tensorboardX.SummaryWriter(log_dir = savedir)\n",
    "        \n",
    "        self.G.to(device)\n",
    "        self.D.to(device)\n",
    "        \n",
    "        self.labeled = labeled\n",
    "        self.unlabeled = unlabeled\n",
    "        self.test = test\n",
    "        self.log_interval = log_interval\n",
    "        self.unlabel_weight = unlabel_weight\n",
    "        self.eval_interval = eval_interval\n",
    "        \n",
    "        self.Doptim = optim.Adam(self.D.parameters(), lr=lr, betas= (momentum, 0.999))\n",
    "        self.Goptim = optim.Adam(self.G.parameters(), lr=lr, betas = (momentum,0.999))\n",
    "        \n",
    "        self.use_cuda = True if device == 'cuda' else False\n",
    "        self.batch_size = batchsize\n",
    "        \n",
    "        \n",
    "    def trainD(self, x_label, y, x_unlabel):\n",
    "        \n",
    "        x_label, x_unlabel, y = Variable(x_label), Variable(x_unlabel), Variable(y, requires_grad = False)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            x_label, x_unlabel, y = x_label.cuda(), x_unlabel.cuda(), y.cuda()\n",
    "            \n",
    "        output_label = self.D(x_label, cuda = self.use_cuda)\n",
    "        output_unlabel = self.D(x_unlabel, cuda = self.use_cuda)\n",
    "        output_fake = self.D(self.G(x_unlabel.size()[0], self.use_cuda).view(x_unlabel.size()).detach(), cuda = self.use_cuda)\n",
    "        \n",
    "        logz_label = log_sum_exp(output_label)\n",
    "        logz_unlabel = log_sum_exp(output_unlabel)\n",
    "        logz_fake = log_sum_exp(output_fake) # log ∑e^x_i\n",
    "\n",
    "        prob_label = torch.gather(output_label, 1, y.unsqueeze(1)) # log e^x_label = x_label \n",
    "        loss_supervised = -torch.mean(prob_label) + torch.mean(logz_label)\n",
    "        loss_unsupervised = 0.5 * (-torch.mean(logz_unlabel) + torch.mean(F.softplus(logz_unlabel))  + # real_data: log Z/(1+Z)\n",
    "                            torch.mean(F.softplus(logz_fake)) ) # fake_data: log 1/(1+Z)\n",
    "        loss = loss_supervised + self.unlabel_weight * loss_unsupervised\n",
    "        acc = torch.mean((output_label.max(1)[1] == y).float())\n",
    "        self.Doptim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Doptim.step()\n",
    "        \n",
    "        return loss_supervised.data.cpu().numpy(), loss_unsupervised.data.cpu().numpy(), acc\n",
    "    \n",
    "    def trainG(self, x_unlabel):\n",
    "        fake = self.G(x_unlabel.size()[0], self.use_cuda).view(x_unlabel.size())\n",
    "        #fake.retain_grad()\n",
    "        mom_gen, output_fake = self.D(fake, feature=True, cuda = self.use_cuda)\n",
    "        mom_unlabel, _ = self.D(Variable(x_unlabel), feature=True, cuda = self.use_cuda)\n",
    "        mom_gen = torch.mean(mom_gen, dim = 0)\n",
    "        mom_unlabel = torch.mean(mom_unlabel, dim = 0)\n",
    "        loss_fm = torch.mean((mom_gen - mom_unlabel) ** 2)\n",
    "        #loss_adv = -torch.mean(F.softplus(log_sum_exp(output_fake)))\n",
    "        loss = loss_fm #+ 1. * loss_adv        \n",
    "        self.Goptim.zero_grad()\n",
    "        self.Doptim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Goptim.step()\n",
    "        return loss.data.cpu().numpy()\n",
    "\n",
    "    def train(self, epochs):\n",
    "        assert self.unlabeled.__len__() > self.labeled.__len__()\n",
    "        assert type(self.labeled) == TensorDataset\n",
    "        times = int(np.ceil(self.unlabeled.__len__() * 1. / self.labeled.__len__()))\n",
    "        t1 = self.labeled.tensors[0].clone()\n",
    "        t2 = self.labeled.tensors[1].clone()\n",
    "        tile_labeled = TensorDataset(t1.repeat(times, 1, 1, 1),t2.repeat(times))\n",
    "        gn = 0\n",
    "        for epoch in range(epochs):\n",
    "            self.G.train()\n",
    "            self.D.train()\n",
    "            unlabel_loader1 = DataLoader(self.unlabeled, batch_size = self.batch_size, shuffle=True, drop_last=True, num_workers = 4)\n",
    "            unlabel_loader2 = DataLoader(self.unlabeled, batch_size = self.batch_size, shuffle=True, drop_last=True, num_workers = 4).__iter__()\n",
    "            label_loader = DataLoader(tile_labeled, batch_size = self.batch_size, shuffle=True, drop_last=True, num_workers = 4).__iter__()\n",
    "            loss_supervised = loss_unsupervised = loss_gen = accuracy = 0.\n",
    "            batch_num = 0\n",
    "            for (unlabel1, _label1) in unlabel_loader1:\n",
    "                #pdb.set_trace()\n",
    "                batch_num += 1\n",
    "                unlabel2, _label2 = unlabel_loader2.next()\n",
    "                x, y = label_loader.next()\n",
    "                if self.use_cuda:\n",
    "                    x, y, unlabel1, unlabel2 = x.cuda(), y.cuda(), unlabel1.cuda(), unlabel2.cuda()\n",
    "                ll, lu, acc = self.trainD(x, y, unlabel1)\n",
    "                loss_supervised += ll\n",
    "                loss_unsupervised += lu\n",
    "                accuracy += acc\n",
    "                lg = self.trainG(unlabel2)\n",
    "                if epoch > 1 and lg > 1:\n",
    "                    #pdb.set_trace()\n",
    "                    lg = self.trainG(unlabel2)\n",
    "                loss_gen += lg\n",
    "                if (batch_num + 1) % self.log_interval == 0:\n",
    "                    #print('Training: %d / %d' % (batch_num + 1, len(unlabel_loader1)))\n",
    "                    gn += 1\n",
    "                    self.writer.add_scalars('loss', {'loss_supervised':ll, 'loss_unsupervised':lu, 'loss_gen':lg}, gn)\n",
    "                    self.writer.add_histogram('real_feature', self.D(Variable(x, volatile = True), cuda=self.use_cuda, feature = True)[0], gn)\n",
    "                    self.writer.add_histogram('fake_feature', self.D(self.G(self.batch_size, cuda = self.use_cuda), cuda=self.use_cuda, feature = True)[0], gn)\n",
    "                    self.writer.add_histogram('fc3_bias', self.G.fc3.bias, gn)\n",
    "                    self.writer.add_histogram('D_feature_weight', self.D.layers[-1].weight, gn)\n",
    "                    #self.writer.add_histogram('D_feature_bias', self.D.layers[-1].bias, gn)\n",
    "                    #print('Eval: correct %d/%d, %.4f' % (self.eval(), self.test.__len__(), acc))\n",
    "                    self.D.train()\n",
    "                    self.G.train()\n",
    "            loss_supervised /= batch_num\n",
    "            loss_unsupervised /= batch_num\n",
    "            loss_gen /= batch_num\n",
    "            accuracy /= batch_num\n",
    "            print(\"Iteration %d, loss_supervised = %.4f, loss_unsupervised = %.4f, loss_gen = %.4f train acc = %.4f\" % (epoch, loss_supervised, loss_unsupervised, loss_gen, accuracy))\n",
    "            sys.stdout.flush()\n",
    "            if (epoch + 1) % self.eval_interval == 0:\n",
    "                print(\"Eval: correct %d / %d\"  % (self.eval(), self.test.__len__()))\n",
    "                torch.save(self.G, os.path.join(self.savedir, 'G.pkl'))\n",
    "                torch.save(self.D, os.path.join(self.savedir, 'D.pkl'))\n",
    "                \n",
    "\n",
    "    def predict(self, x):\n",
    "        return torch.max(self.D(Variable(x, volatile=True), cuda=self.use_cuda), 1)[1].data\n",
    "    \n",
    "    def eval(self):\n",
    "        self.G.eval()\n",
    "        self.D.eval()\n",
    "        d, l = [], []\n",
    "        for (datum, label) in self.test:\n",
    "            d.append(datum)\n",
    "            l.append(label)\n",
    "        x, y = torch.stack(d), torch.LongTensor(l)\n",
    "        if self.use_cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        pred = self.predict(x)\n",
    "        return torch.sum(pred == y)\n",
    "    \n",
    "    def draw(self, batch_size):\n",
    "        self.G.eval()\n",
    "        return self.G(batch_size, cuda=self.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim = 3 * 32 ** 2, output_dim = 10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            LinearWeightNorm(input_dim, 1000),\n",
    "            LinearWeightNorm(1000, 500),\n",
    "            LinearWeightNorm(500, 250),\n",
    "            LinearWeightNorm(250, 250),\n",
    "            LinearWeightNorm(250, 250)]\n",
    "        )\n",
    "        self.final = LinearWeightNorm(250, output_dim, weight_scale=1)\n",
    "        #for layer in self.layers:\n",
    "        #    reset_normal_param(layer, 0.1)\n",
    "        #reset_normal_param(self.final, 0.1, 5)\n",
    "    def forward(self, x, feature = False, cuda = False):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        noise = torch.randn(x.size()) * 0.3 if self.training else torch.Tensor([0])\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        x = x + Variable(noise, requires_grad = False)\n",
    "        for i in range(len(self.layers)):\n",
    "            m = self.layers[i]\n",
    "            x_f = F.relu(m(x))\n",
    "            noise = torch.randn(x_f.size()) * 0.5 if self.training else torch.Tensor([0])\n",
    "            if cuda:\n",
    "                noise = noise.cuda()\n",
    "            x = (x_f + Variable(noise, requires_grad = False))\n",
    "        if feature:\n",
    "            return x_f, self.final(x)\n",
    "        return self.final(x)\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, output_dim = 3 * 32 ** 2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.fc1 = nn.Linear(z_dim, 500, bias = False)\n",
    "        self.bn1 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
    "        self.fc2 = nn.Linear(500, 500, bias = False)\n",
    "        self.bn2 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
    "        self.fc3 = LinearWeightNorm(500, output_dim, weight_scale = 1)\n",
    "        self.bn1_b = nn.Parameter(torch.zeros(500))\n",
    "        self.bn2_b = nn.Parameter(torch.zeros(500))\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        #reset_normal_param(self.fc1, 0.1)\n",
    "        #reset_normal_param(self.fc2, 0.1)\n",
    "        #reset_normal_param(self.fc3, 0.1)\n",
    "    def forward(self, batch_size, cuda = False):\n",
    "        x = Variable(torch.rand(batch_size, self.z_dim), requires_grad = False, volatile = not self.training)\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        x = F.softplus(self.bn1(self.fc1(x)) + self.bn1_b)\n",
    "        x = F.softplus(self.bn2(self.fc2(x)) + self.bn2_b)\n",
    "        x = F.softplus(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(36),\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CIFAO10Label(transformer, class_num = 10):\n",
    "    raw_dataset = CIFAR10(\"../data\", train = 1, max_num = 100, transform = transformer)\n",
    "    class_tot = [0] * 10\n",
    "    data = []\n",
    "    labels = []\n",
    "    positive_tot = 0\n",
    "    tot = 0\n",
    "    perm = np.random.permutation(raw_dataset.__len__())\n",
    "    for i in range(raw_dataset.__len__()):\n",
    "        datum, label = raw_dataset.__getitem__(perm[i])\n",
    "        if class_tot[label] < class_num:\n",
    "            data.append(datum.numpy())\n",
    "            labels.append(label)\n",
    "            class_tot[label] += 1\n",
    "            tot += 1\n",
    "            if tot >= 10 * class_num:\n",
    "                break\n",
    "    return TensorDataset(torch.FloatTensor(np.array(data)), torch.LongTensor(np.array(labels)))\n",
    "\n",
    "labelset = CIFAO10Label(transformer = transform_train)\n",
    "unlabelset = CIFAR10(\"../data\", train = -1, max_num = 50000, transform = transform_train)\n",
    "testset = CIFAR10(\"../data\", train = 0, transform = transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = ImprovedGAN(Generator(200), Discriminator(), labelset, unlabelset, testset, eval_interval = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkq/anaconda3/envs/objet/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss_supervised = 0.4802, loss_unsupervised = 0.6779, loss_gen = 0.2459 train acc = 0.8520\n",
      "Iteration 1, loss_supervised = 0.0313, loss_unsupervised = 0.5754, loss_gen = 0.1850 train acc = 0.9907\n",
      "Iteration 2, loss_supervised = 0.0053, loss_unsupervised = 0.5510, loss_gen = 0.1815 train acc = 0.9987\n",
      "Iteration 3, loss_supervised = 0.1075, loss_unsupervised = 0.6064, loss_gen = 0.1511 train acc = 0.9695\n",
      "Iteration 4, loss_supervised = 0.0033, loss_unsupervised = 0.5957, loss_gen = 0.1366 train acc = 0.9997\n",
      "Eval: correct 1536 / 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkq/anaconda3/envs/objet/lib/python3.6/site-packages/ipykernel_launcher.py:131: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss_supervised = 0.1382, loss_unsupervised = 0.6483, loss_gen = 0.1351 train acc = 0.9683\n",
      "Iteration 6, loss_supervised = 0.0027, loss_unsupervised = 0.5964, loss_gen = 0.1584 train acc = 0.9998\n",
      "Iteration 7, loss_supervised = 0.0119, loss_unsupervised = 0.5984, loss_gen = 0.1369 train acc = 0.9971\n",
      "Iteration 8, loss_supervised = 0.0023, loss_unsupervised = 0.5725, loss_gen = 0.1706 train acc = 0.9996\n",
      "Iteration 9, loss_supervised = 0.0010, loss_unsupervised = 0.5666, loss_gen = 0.1811 train acc = 1.0000\n",
      "Eval: correct 1501 / 10000\n",
      "Iteration 10, loss_supervised = 0.0074, loss_unsupervised = 0.5666, loss_gen = 0.1619 train acc = 0.9981\n",
      "Iteration 11, loss_supervised = 0.0308, loss_unsupervised = 0.5872, loss_gen = 0.1793 train acc = 0.9920\n",
      "Iteration 12, loss_supervised = 0.0006, loss_unsupervised = 0.5544, loss_gen = 0.2052 train acc = 1.0000\n",
      "Iteration 13, loss_supervised = 0.0750, loss_unsupervised = 0.5994, loss_gen = 0.2482 train acc = 0.9804\n",
      "Iteration 14, loss_supervised = 0.0012, loss_unsupervised = 0.5735, loss_gen = 0.2627 train acc = 1.0000\n",
      "Eval: correct 1462 / 10000\n",
      "Iteration 15, loss_supervised = 0.0009, loss_unsupervised = 0.5472, loss_gen = 0.3070 train acc = 0.9999\n",
      "Iteration 16, loss_supervised = 0.0514, loss_unsupervised = 0.5930, loss_gen = 0.3059 train acc = 0.9862\n",
      "Iteration 17, loss_supervised = 0.0009, loss_unsupervised = 0.5473, loss_gen = 0.3512 train acc = 1.0000\n",
      "Iteration 18, loss_supervised = 0.0005, loss_unsupervised = 0.5352, loss_gen = 0.4091 train acc = 1.0000\n",
      "Iteration 19, loss_supervised = 0.0089, loss_unsupervised = 0.5428, loss_gen = 0.3406 train acc = 0.9976\n",
      "Eval: correct 1404 / 10000\n",
      "Iteration 20, loss_supervised = 0.0004, loss_unsupervised = 0.5235, loss_gen = 0.4316 train acc = 1.0000\n",
      "Iteration 21, loss_supervised = 0.0054, loss_unsupervised = 0.5513, loss_gen = 0.3700 train acc = 0.9988\n",
      "Iteration 22, loss_supervised = 0.0002, loss_unsupervised = 0.5343, loss_gen = 0.4536 train acc = 1.0000\n",
      "Iteration 23, loss_supervised = 0.1510, loss_unsupervised = 0.6518, loss_gen = 0.4656 train acc = 0.9718\n",
      "Iteration 24, loss_supervised = 0.0025, loss_unsupervised = 0.5754, loss_gen = 0.5036 train acc = 0.9998\n",
      "Eval: correct 1323 / 10000\n",
      "Iteration 25, loss_supervised = 0.0014, loss_unsupervised = 0.5268, loss_gen = 0.5493 train acc = 1.0000\n",
      "Iteration 26, loss_supervised = 0.0013, loss_unsupervised = 0.5134, loss_gen = 0.5928 train acc = 0.9999\n",
      "Iteration 27, loss_supervised = 0.0053, loss_unsupervised = 0.5114, loss_gen = 0.5475 train acc = 0.9988\n",
      "Iteration 28, loss_supervised = 0.0006, loss_unsupervised = 0.5188, loss_gen = 0.5836 train acc = 1.0000\n",
      "Iteration 29, loss_supervised = 0.0004, loss_unsupervised = 0.5195, loss_gen = 0.6371 train acc = 1.0000\n",
      "Eval: correct 1310 / 10000\n",
      "Iteration 30, loss_supervised = 0.0003, loss_unsupervised = 0.5226, loss_gen = 0.6776 train acc = 1.0000\n",
      "Iteration 31, loss_supervised = 0.0450, loss_unsupervised = 0.5827, loss_gen = 0.5846 train acc = 0.9892\n",
      "Iteration 32, loss_supervised = 0.0009, loss_unsupervised = 0.5489, loss_gen = 0.5679 train acc = 1.0000\n",
      "Iteration 33, loss_supervised = 0.0004, loss_unsupervised = 0.5251, loss_gen = 0.7097 train acc = 1.0000\n",
      "Iteration 34, loss_supervised = 0.0003, loss_unsupervised = 0.5180, loss_gen = 0.7621 train acc = 1.0000\n",
      "Eval: correct 1373 / 10000\n",
      "Iteration 35, loss_supervised = 0.0020, loss_unsupervised = 0.5235, loss_gen = 0.7254 train acc = 0.9995\n",
      "Iteration 36, loss_supervised = 0.0002, loss_unsupervised = 0.5275, loss_gen = 0.7368 train acc = 1.0000\n",
      "Iteration 37, loss_supervised = 0.0002, loss_unsupervised = 0.5268, loss_gen = 0.7758 train acc = 1.0000\n",
      "Iteration 38, loss_supervised = 0.0002, loss_unsupervised = 0.5254, loss_gen = 0.7990 train acc = 1.0000\n",
      "Iteration 39, loss_supervised = 0.1031, loss_unsupervised = 0.5909, loss_gen = 0.7829 train acc = 0.9833\n",
      "Eval: correct 1411 / 10000\n",
      "Iteration 40, loss_supervised = 0.0007, loss_unsupervised = 0.5515, loss_gen = 0.7750 train acc = 1.0000\n",
      "Iteration 41, loss_supervised = 0.0004, loss_unsupervised = 0.5416, loss_gen = 0.8524 train acc = 1.0000\n",
      "Iteration 42, loss_supervised = 0.0003, loss_unsupervised = 0.5361, loss_gen = 0.9337 train acc = 1.0000\n",
      "Iteration 43, loss_supervised = 0.0003, loss_unsupervised = 0.5362, loss_gen = 0.9293 train acc = 1.0000\n",
      "Iteration 44, loss_supervised = 0.0003, loss_unsupervised = 0.5408, loss_gen = 0.9318 train acc = 1.0000\n",
      "Eval: correct 1501 / 10000\n",
      "Iteration 45, loss_supervised = 0.0002, loss_unsupervised = 0.5437, loss_gen = 0.9524 train acc = 1.0000\n",
      "Iteration 46, loss_supervised = 0.0430, loss_unsupervised = 0.5978, loss_gen = 0.7705 train acc = 0.9922\n",
      "Iteration 47, loss_supervised = 0.0007, loss_unsupervised = 0.5560, loss_gen = 0.8767 train acc = 1.0000\n",
      "Iteration 48, loss_supervised = 0.0004, loss_unsupervised = 0.5523, loss_gen = 0.9375 train acc = 1.0000\n",
      "Iteration 49, loss_supervised = 0.0009, loss_unsupervised = 0.5459, loss_gen = 0.9610 train acc = 0.9998\n",
      "Eval: correct 1372 / 10000\n",
      "Iteration 50, loss_supervised = 0.0002, loss_unsupervised = 0.5436, loss_gen = 0.9490 train acc = 1.0000\n",
      "Iteration 51, loss_supervised = 0.0002, loss_unsupervised = 0.5443, loss_gen = 1.0199 train acc = 1.0000\n",
      "Iteration 52, loss_supervised = 0.0058, loss_unsupervised = 0.5541, loss_gen = 0.9611 train acc = 0.9986\n",
      "Iteration 53, loss_supervised = 0.0018, loss_unsupervised = 0.5557, loss_gen = 0.9422 train acc = 0.9997\n",
      "Iteration 54, loss_supervised = 0.0001, loss_unsupervised = 0.5437, loss_gen = 1.0420 train acc = 1.0000\n",
      "Eval: correct 1528 / 10000\n",
      "Iteration 55, loss_supervised = 0.0007, loss_unsupervised = 0.5469, loss_gen = 1.0236 train acc = 0.9999\n",
      "Iteration 56, loss_supervised = 0.0001, loss_unsupervised = 0.5468, loss_gen = 1.0452 train acc = 1.0000\n",
      "Iteration 57, loss_supervised = 0.0002, loss_unsupervised = 0.5479, loss_gen = 1.0333 train acc = 1.0000\n",
      "Iteration 58, loss_supervised = 0.0061, loss_unsupervised = 0.5513, loss_gen = 1.0100 train acc = 0.9987\n",
      "Iteration 59, loss_supervised = 0.0002, loss_unsupervised = 0.5546, loss_gen = 0.9014 train acc = 1.0000\n",
      "Eval: correct 1382 / 10000\n",
      "Iteration 60, loss_supervised = 0.0001, loss_unsupervised = 0.5482, loss_gen = 1.0165 train acc = 1.0000\n",
      "Iteration 61, loss_supervised = 0.0001, loss_unsupervised = 0.5445, loss_gen = 1.0682 train acc = 1.0000\n",
      "Iteration 62, loss_supervised = 0.0001, loss_unsupervised = 0.5439, loss_gen = 1.0428 train acc = 1.0000\n",
      "Iteration 63, loss_supervised = 0.0001, loss_unsupervised = 0.5473, loss_gen = 1.0347 train acc = 1.0000\n",
      "Iteration 64, loss_supervised = 0.0456, loss_unsupervised = 0.6088, loss_gen = 0.8875 train acc = 0.9894\n",
      "Eval: correct 1519 / 10000\n",
      "Iteration 65, loss_supervised = 0.0010, loss_unsupervised = 0.5712, loss_gen = 0.8952 train acc = 0.9999\n",
      "Iteration 66, loss_supervised = 0.0005, loss_unsupervised = 0.5633, loss_gen = 1.0698 train acc = 1.0000\n",
      "Iteration 67, loss_supervised = 0.0002, loss_unsupervised = 0.5527, loss_gen = 1.1585 train acc = 1.0000\n",
      "Iteration 68, loss_supervised = 0.0001, loss_unsupervised = 0.5497, loss_gen = 1.2240 train acc = 1.0000\n",
      "Iteration 69, loss_supervised = 0.0002, loss_unsupervised = 0.5542, loss_gen = 1.1820 train acc = 1.0000\n",
      "Eval: correct 1407 / 10000\n",
      "Iteration 70, loss_supervised = 0.0001, loss_unsupervised = 0.5534, loss_gen = 1.1684 train acc = 1.0000\n",
      "Iteration 71, loss_supervised = 0.0001, loss_unsupervised = 0.5568, loss_gen = 1.1422 train acc = 1.0000\n",
      "Iteration 72, loss_supervised = 0.0001, loss_unsupervised = 0.5536, loss_gen = 1.1319 train acc = 1.0000\n",
      "Iteration 73, loss_supervised = 0.0001, loss_unsupervised = 0.5594, loss_gen = 1.0702 train acc = 1.0000\n",
      "Iteration 74, loss_supervised = 0.0274, loss_unsupervised = 0.5865, loss_gen = 1.0883 train acc = 0.9935\n",
      "Eval: correct 1482 / 10000\n",
      "Iteration 75, loss_supervised = 0.0002, loss_unsupervised = 0.5657, loss_gen = 1.2226 train acc = 1.0000\n",
      "Iteration 76, loss_supervised = 0.0001, loss_unsupervised = 0.5586, loss_gen = 1.2919 train acc = 1.0000\n",
      "Iteration 77, loss_supervised = 0.0001, loss_unsupervised = 0.5561, loss_gen = 1.2975 train acc = 1.0000\n",
      "Iteration 78, loss_supervised = 0.0001, loss_unsupervised = 0.5560, loss_gen = 1.2390 train acc = 1.0000\n",
      "Iteration 79, loss_supervised = 0.0001, loss_unsupervised = 0.5579, loss_gen = 1.2214 train acc = 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: correct 1379 / 10000\n",
      "Iteration 80, loss_supervised = 0.0001, loss_unsupervised = 0.5588, loss_gen = 1.2184 train acc = 1.0000\n",
      "Iteration 81, loss_supervised = 0.0001, loss_unsupervised = 0.5609, loss_gen = 1.1777 train acc = 1.0000\n",
      "Iteration 82, loss_supervised = 0.0179, loss_unsupervised = 0.5838, loss_gen = 1.0740 train acc = 0.9950\n",
      "Iteration 83, loss_supervised = 0.0001, loss_unsupervised = 0.5714, loss_gen = 1.1056 train acc = 1.0000\n",
      "Iteration 84, loss_supervised = 0.0001, loss_unsupervised = 0.5621, loss_gen = 1.2377 train acc = 1.0000\n",
      "Eval: correct 1356 / 10000\n",
      "Iteration 85, loss_supervised = 0.0001, loss_unsupervised = 0.5560, loss_gen = 1.2378 train acc = 1.0000\n",
      "Iteration 86, loss_supervised = 0.0001, loss_unsupervised = 0.5628, loss_gen = 1.1886 train acc = 1.0000\n",
      "Iteration 87, loss_supervised = 0.0001, loss_unsupervised = 0.5615, loss_gen = 1.1944 train acc = 1.0000\n",
      "Iteration 88, loss_supervised = 0.0001, loss_unsupervised = 0.5633, loss_gen = 1.1822 train acc = 1.0000\n",
      "Iteration 89, loss_supervised = 0.0196, loss_unsupervised = 0.5965, loss_gen = 1.0825 train acc = 0.9953\n",
      "Eval: correct 1544 / 10000\n",
      "Iteration 90, loss_supervised = 0.0006, loss_unsupervised = 0.5770, loss_gen = 1.3244 train acc = 1.0000\n",
      "Iteration 91, loss_supervised = 0.0002, loss_unsupervised = 0.5603, loss_gen = 1.4307 train acc = 1.0000\n",
      "Iteration 92, loss_supervised = 0.0001, loss_unsupervised = 0.5582, loss_gen = 1.3809 train acc = 1.0000\n",
      "Iteration 93, loss_supervised = 0.0001, loss_unsupervised = 0.5586, loss_gen = 1.3310 train acc = 1.0000\n",
      "Iteration 94, loss_supervised = 0.0001, loss_unsupervised = 0.5598, loss_gen = 1.2451 train acc = 1.0000\n",
      "Eval: correct 1508 / 10000\n",
      "Iteration 95, loss_supervised = 0.0001, loss_unsupervised = 0.5626, loss_gen = 1.2028 train acc = 1.0000\n",
      "Iteration 96, loss_supervised = 0.0001, loss_unsupervised = 0.5628, loss_gen = 1.1806 train acc = 1.0000\n",
      "Iteration 97, loss_supervised = 0.0151, loss_unsupervised = 0.5771, loss_gen = 1.1377 train acc = 0.9963\n",
      "Iteration 98, loss_supervised = 0.0002, loss_unsupervised = 0.5639, loss_gen = 1.0981 train acc = 1.0000\n",
      "Iteration 99, loss_supervised = 0.0001, loss_unsupervised = 0.5616, loss_gen = 1.1409 train acc = 1.0000\n",
      "Eval: correct 1620 / 10000\n"
     ]
    }
   ],
   "source": [
    "gan.train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_set = CIFAR10(\"../data\", train = -1, max_num = 60000, transform = transform_test)\n",
    "cifarloader = torch.utils.data.DataLoader(cifar_set, batch_size = 64, shuffle = False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch_gan(net, testloader, use_cuda):\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "        \n",
    "    with trange(len(testloader), file=sys.stderr) as t:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs, cuda=use_cuda)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                t.update()\n",
    "\n",
    "        t.set_postfix(test_loss = test_loss / total, test_acc = correct * 100. / total)\n",
    "        t.update(1)\n",
    "            \n",
    "    return correct * 100. / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "939it [00:07, 123.11it/s, test_acc=15.4, test_loss=0.0678]                         \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_score = val_epoch_gan(gan.D, cifarloader, gan.use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Open) Question 10:__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort.\n",
    "\n",
    "> We trained a GAN model taking as input both labeled images and unlabeled images and the results on shown as below. One of the disadvantage of the model is that both the discriminator and the generator do not contain convolutional layers.\n",
    "\n",
    "| Model | Number of  epochs  | Train accuracy | Test accuracy | Accuracy on Full Data | Reference github|\n",
    "|------|------|------|------|------|------|\n",
    "|  Weak Supervised Learning  | 100 | 100 | 16.20 | 15.40 | https://github.com/Sleepychord/ImprovedGAN-pytorch |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 11:__ Write a short report explaining the pros and the cons of each methods that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
